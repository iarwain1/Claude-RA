# Prompt Engineering and the Effectiveness of Large Language Models in Enhancing Human Productivity

**Authors:** Rizal Khoirul Anam (Nanjing University of Information Science and Technology)
**Year:** 2025 (May)
**URL:** https://arxiv.org/abs/2507.18638

## Key Contributions

1. **243-respondent survey** on prompt engineering effectiveness
2. **Cross-domain sample:** Learners, workers, educators, freelancers
3. **Correlation established:** Clear, structured, context-aware prompts → higher task efficiency
4. **Gap identified:** Most users rely on trial-and-error, lacking explicit training

## Methodology

- **Survey instrument:** Google Forms distributed across domains
- **Sample size:** 243 respondents
- **Demographics:** Academic and occupational diversity (learners, workers, educators, freelancers)
- **Measures:** AI usage habits, prompting strategies, user satisfaction

## Key Findings

### Prompt Quality → Outcome Quality

> "Users who employ clear, structured, and context-aware prompts report higher task efficiency and better outcomes."

**Prompt characteristics correlated with success:**
- Clarity of instructions
- Structured format
- Context-awareness (relevant background information)

### Current User Behavior Gap

> "End users continue to query LLMs with imprecise or generic prompts and remain oblivious to how much influence design of prompts has on results. Lacking explicit training or instruction, users adapt by trial and error."

**Problems with trial-and-error:**
- Inconsistency of results
- Reduced productivity advantage
- Suboptimal use of tool capabilities

### Implications for AI Literacy

> "As LLMs find increasing application in education, business, and everyday life, prompt engineering is becoming an essential digital competence."

**Applications:**
- AI literacy initiatives
- Tool design improvements
- Promoting wiser AI use

## Relevance to Review

**Moderately relevant for human factors in prompt evaluation.** Key insights:

### For Evaluation Design

1. **User prompt quality varies widely** - Evaluation systems may need to account for this
2. **Training gap exists** - Many users don't know prompting best practices
3. **Self-reported satisfaction correlates with prompt structure** - But is self-report valid measure?

### Implications for Different Prompt Types

| Prompt Type | Survey Implication |
|-------------|-------------------|
| User prompts (consumer products) | High variability in quality expected |
| System prompts (bespoke systems) | Professional training may reduce variability |
| Educational contexts | Prompt training has measurable impact |

### Evaluation System Design

Survey findings suggest:
- Need to **evaluate prompt quality** as input variable, not just output quality
- **User training** may improve overall system performance
- **Tool design** can guide users toward better prompts

### Limitations for This Review

| Finding | Limitation |
|---------|-----------|
| Self-reported satisfaction | May not correlate with objective quality |
| Survey methodology | No objective measurement of output quality |
| Cross-domain sample | May obscure domain-specific patterns |

## Citations to Follow

- Related user studies on AI interaction patterns
- Prompt engineering training interventions
- LLM output quality measurement methodologies

## Questions/Notes

- **Strength:** Large sample size (243 respondents)
- **Strength:** Cross-domain perspective
- **Strength:** Identifies real user behavior gap (trial-and-error)
- **Limitation:** Self-reported outcomes, not objective measurement
- **Limitation:** Survey design may have response bias
- **Gap:** No comparison of specific prompting strategies
- **Gap:** No objective quality evaluation of outputs
- **Connects to:** prompting2025practice (complementary empirical study), human-evaluation subtopic
- **Currency:** May 2025, current

## Evidence Quality

**Medium for self-reported findings.** Survey methodology with reasonable sample size. Good for "what users report" patterns; weaker for causal claims about prompt quality → output quality. Self-reported satisfaction is imperfect proxy for actual effectiveness.
