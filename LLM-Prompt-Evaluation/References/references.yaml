# References for LLM Prompt Evaluation
# Created: 2025-12-17
# Last updated: 2025-12-17

review_name: "LLM Prompt Evaluation"
topic: "Methods for evaluating the effectiveness of prompts in LLM-based systems"
created: "2025-12-17"
last_updated: "2025-12-17"

references:
  # === PRACTITIONER GUIDES (Hamel Husain) ===

  - key: husain2024evals
    title: "Your AI Product Needs Evals"
    authors: ["Husain, Hamel"]
    year: 2024
    url: "https://hamel.dev/blog/posts/evals/"
    abstract: "Comprehensive guide to LLM evaluation covering three levels: Level 1 (unit tests/assertions), Level 2 (model-based evaluation with human correlation tracking), and Level 3 (A/B testing for mature products). Emphasizes error analysis as the foundation for effective evals."
    source: blog
    priority: 10
    subtopics: ["evaluation-frameworks", "practical-guidance"]
    read: true
    notes_file: "Claude-Notes/Paper-Summaries/husain2024evals.md"
    added: "2025-12-17"

  - key: husain2024llmjudge
    title: "Using LLM-as-a-Judge For Evaluation: A Complete Guide"
    authors: ["Husain, Hamel"]
    year: 2024
    url: "https://hamel.dev/blog/posts/llm-judge/"
    abstract: "Detailed guide on using LLMs to evaluate other LLM outputs. Key insight: must measure LLM-as-judge agreement with human judgments to validate trustworthiness. Based on experience helping 30+ companies set up evaluation systems."
    source: blog
    priority: 10
    subtopics: ["llm-as-judge", "practical-guidance"]
    read: true
    notes_file: "Claude-Notes/Paper-Summaries/husain2024llmjudge.md"
    added: "2025-12-17"

  - key: husain2025fieldguide
    title: "A Field Guide to Rapidly Improving AI Products"
    authors: ["Husain, Hamel"]
    year: 2025
    url: "https://hamel.dev/blog/posts/field-guide/"
    abstract: "Practical guide based on 30+ production implementations. Warns against the 'tools trap' and generic metrics. Emphasizes custom data viewers, maintaining trust in evaluations, and systematic iteration over tool selection."
    source: blog
    priority: 9
    subtopics: ["evaluation-frameworks", "practical-guidance"]
    read: true
    notes_file: "Claude-Notes/Paper-Summaries/husain2025fieldguide.md"
    added: "2025-12-17"

  - key: husain2025evalsfaq
    title: "AI Evals FAQ"
    authors: ["Husain, Hamel", "Shankar, Shreya"]
    year: 2025
    url: "https://hamel.dev/blog/posts/evals-faq/"
    abstract: "Collection of FAQs covering: what are LLM evals, evaluating agentic workflows, RAG evaluation, multi-step workflows, error analysis methodology, eval-driven development, model selection, and synthetic data generation."
    source: blog
    priority: 10
    subtopics: ["evaluation-frameworks", "agentic-evaluation", "practical-guidance"]
    read: true
    notes_file: "Claude-Notes/Paper-Summaries/husain2025evalsfaq.md"
    added: "2025-12-17"

  # === ANTHROPIC RESOURCES ===

  - key: anthropic2025evaltool
    title: "Using the Evaluation Tool"
    authors: ["Anthropic"]
    year: 2025
    url: "https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool"
    abstract: "Documentation for Anthropic Console's Evaluation tool. Covers manual test cases, auto-generated test cases, CSV import, prompt versioning, and iterative testing workflow."
    source: web
    priority: 8
    subtopics: ["automated-metrics", "practical-guidance"]
    read: true
    notes_file: "Claude-Notes/Paper-Summaries/anthropic2025evaltool.md"
    added: "2025-12-17"

  - key: anthropic2025promptengineering
    title: "Prompt Engineering Overview"
    authors: ["Anthropic"]
    year: 2025
    url: "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview"
    abstract: "Overview of prompt engineering with guidance on success criteria controllable through prompting vs. other factors like model selection, latency, and cost optimization."
    source: web
    priority: 7
    subtopics: ["evaluation-frameworks", "practical-guidance"]
    read: true
    notes_file: "Claude-Notes/Paper-Summaries/anthropic2025promptengineering.md"
    added: "2025-12-17"

  - key: anthropic2025claude4bestpractices
    title: "Claude 4 Best Practices"
    authors: ["Anthropic"]
    year: 2025
    url: "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/claude-4-best-practices.md"
    abstract: "Specific prompt engineering techniques for Claude 4 models (Opus 4.1, Opus 4, Sonnet 4). Emphasizes explicit instructions and precise instruction following."
    source: web
    priority: 6
    subtopics: ["practical-guidance"]
    read: false
    notes_file: null
    added: "2025-12-17"

  - key: anthropic2025toolsforagents
    title: "Writing Tools for Agents"
    authors: ["Anthropic"]
    year: 2025
    url: "https://www.anthropic.com/engineering/writing-tools-for-agents"
    abstract: "Engineering blog post on tool evaluation best practices. Each evaluation prompt should be paired with verifiable responses. Recommends realistic data sources over 'sandbox' environments."
    source: blog
    priority: 8
    subtopics: ["agentic-evaluation", "practical-guidance"]
    read: true
    notes_file: "Claude-Notes/Paper-Summaries/anthropic2025toolsforagents.md"
    added: "2025-12-17"

  - key: anthropic2025promptimprover
    title: "Use our prompt improver to optimize your prompts"
    authors: ["Anthropic"]
    year: 2025
    url: "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-improver"
    abstract: "Tool for automatic prompt refinement using Claude. Adds chain-of-thought instructions, XML tag organization, and standardized example formatting."
    source: web
    priority: 6
    subtopics: ["prompt-optimization"]
    read: false
    notes_file: null
    added: "2025-12-17"

  # === ARXIV PAPERS (CONFIRMED DETAILS) ===

  - key: guo2025agentbenchmark
    title: "A Comprehensive Survey on Benchmarks and Solutions in Software Engineering of LLM-Empowered Agentic System"
    authors: ["Guo, Jiale", "Huang, Suizhi", "Li, Mei", "Huang, Dong", "Chen, Xingsheng", "Zhang, Regina", "Guo, Zhijiang", "Yu, Han", "Yiu, Siu-Ming", "Lio, Pietro", "Lam, Kwok-Yan"]
    year: 2025
    url: "https://arxiv.org/abs/2510.09721"
    abstract: "Comprehensive survey reviewing 150+ papers on LLM-powered software engineering. Proposes taxonomy along Solutions (prompt-based, fine-tuning-based, agent-based) and Benchmarks (code generation, translation, repair). Analyzes evolution from prompt engineering to sophisticated agentic systems."
    source: arxiv
    priority: 9
    subtopics: ["evaluation-frameworks", "agentic-evaluation", "automated-metrics"]
    read: true
    notes_file: "Claude-Notes/Paper-Summaries/guo2025agentbenchmark.md"
    added: "2025-12-17"

  - key: ace2025contextengineering
    title: "Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models"
    authors: ["Zhang, Qizheng", "Hu, Changran", "Upasani, Shubhangi", "Ma, Boyuan", "Hong, Fenglu", "Kamanuru, Vamsidhar", "Rainton, Jay", "Wu, Chen", "Ji, Mengmeng", "Li, Hanchen", "Thakker, Urmish", "Zou, James", "Olukotun, Kunle"]
    year: 2025
    url: "https://arxiv.org/abs/2510.04618"
    abstract: "ACE framework treats contexts as evolving playbooks through Generator/Reflector/Curator roles. Addresses brevity bias and context collapse. Results: +10.6% AppWorld, +8.6% finance reasoning, 86.9% latency reduction. No labeled supervision required."
    source: arxiv
    priority: 7
    subtopics: ["prompt-optimization", "agentic-evaluation"]
    read: true
    notes_file: "Claude-Notes/Paper-Summaries/ace2025contextengineering.md"
    added: "2025-12-17"

  - key: survey2025codeintelligence
    title: "From Code Foundation Models to Agents and Applications: A Comprehensive Survey and Practical Guide to Code Intelligence"
    authors: ["Yang, Jian", "et al."]
    year: 2025
    url: "https://arxiv.org/abs/2511.18538"
    abstract: "Comprehensive survey (70+ authors) on code LLMs covering entire model lifecycle. Six-stage evolution: manual → tool-assisted → framework-driven → AI-assisted → AI-driven → AI-autonomous. Performance from single-digit to 95%+ on HumanEval."
    source: arxiv
    priority: 7
    subtopics: ["evaluation-frameworks", "agentic-evaluation", "automated-metrics"]
    read: true
    notes_file: "Claude-Notes/Paper-Summaries/survey2025codeintelligence.md"
    added: "2025-12-17"

  # === ARXIV PAPERS (USER PROVIDED - VERIFIED) ===

  - key: mei2025contextengineering
    title: "A Survey of Context Engineering for Large Language Models"
    authors: ["Mei, Lingrui", "Yao, Jiayu", "Ge, Yuyao", "Wang, Yiwei", "Bi, Baolong", "Cai, Yujun", "Liu, Jiazhi", "Li, Mingyu", "Li, Zhong-Zhi", "Zhang, Duzhen", "Zhou, Chenlin", "Mao, Jiayi", "Xia, Tianze", "Guo, Jiafeng", "Liu, Shenghua"]
    year: 2025
    url: "https://arxiv.org/abs/2507.13334"
    abstract: "Comprehensive survey of 1,400+ papers introducing Context Engineering as a formal discipline. Covers context retrieval/generation, processing, management, and system implementations including RAG, memory systems, tool-integrated reasoning, and multi-agent systems. Key finding: models can understand vast contexts but struggle to generate equally sophisticated outputs."
    source: arxiv
    priority: 9
    subtopics: ["evaluation-frameworks", "agentic-evaluation", "prompt-optimization"]
    read: true
    notes_file: "Claude-Notes/Paper-Summaries/mei2025contextengineering.md"
    added: "2025-12-17"

  - key: contexteng2025evolution
    title: "Context Engineering 2.0: The Context of Context Engineering"
    authors: []
    year: 2025
    url: "https://arxiv.org/abs/2510.26493"
    abstract: "Maps evolution of context engineering through stages: 1.0 Context as Translation (humans adapt to computers), 2.0 Context as Instruction (LLMs interpret natural language), 3.0 Context as Scenario (agents understand goals), 4.0 Context as World (AI proactively builds environment). Traces roots back 20+ years."
    source: arxiv
    priority: 6
    subtopics: ["evaluation-frameworks"]
    read: false
    notes_file: null
    added: "2025-12-17"

  - key: anam2025prompteffectiveness
    title: "Prompt Engineering and the Effectiveness of Large Language Models in Enhancing Human Productivity"
    authors: ["Anam, Rizal Khoirul"]
    year: 2025
    url: "https://arxiv.org/abs/2507.18638"
    abstract: "Survey study of 243 respondents analyzing AI usage habits, prompting strategies, and user satisfaction. Finds users with clear, structured, context-aware prompts report higher task efficiency and better outcomes. Empirical evidence for prompt engineering value."
    source: arxiv
    priority: 7
    subtopics: ["human-evaluation", "practical-guidance"]
    read: true
    notes_file: "Claude-Notes/Paper-Summaries/anam2025prompteffectiveness.md"
    added: "2025-12-17"

  # === ARXIV PAPERS (USER PROVIDED - NOW VERIFIED) ===

  - key: promptcot2025scaling
    title: "PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning"
    authors: ["Zhao, Xueliang", "Wu, Wei", "Guan, Jian", "Gong, Zhuocheng", "Kong, Lingpeng"]
    year: 2025
    url: "https://arxiv.org/abs/2509.19894"
    abstract: "EM-based framework for scaling prompt synthesis. Self-play and SFT regimes. SOTA results: +4.4-5.3 on AIME 24/25, +6.1 on LiveCodeBench. Synthetic prompts outperform human-curated data. 4.8M synthetic prompts."
    source: arxiv
    priority: 7
    subtopics: ["prompt-optimization", "automated-metrics"]
    read: true
    notes_file: "Claude-Notes/Paper-Summaries/promptcot2025scaling.md"
    added: "2025-12-17"

  - key: prompting2025practice
    title: "Prompting in Practice: Investigating Software Developers' Use of Generative AI Tools"
    authors: ["Otten, Daniel", "Stalnaker, Trevor", "Wintersgill, Nathan", "Chaparro, Oscar", "Poshyvanyk, Denys"]
    year: 2025
    url: "https://arxiv.org/abs/2510.06000"
    abstract: "Empirical study of 91 software engineers on GenAI usage. Key findings: iterative multi-turn prompting preferred (10+ turns common), task-specific reliability varies (documentation highest, complex reasoning lowest), strategy usage rates quantified (Output Style 82%, Few-Shot 75%)."
    source: arxiv
    priority: 8
    subtopics: ["human-evaluation", "practical-guidance"]
    read: true
    notes_file: "Claude-Notes/Paper-Summaries/prompting2025practice.md"
    added: "2025-12-17"

  - key: innogym2025benchmark
    title: "InnoGym: Benchmarking the Innovation Potential of AI Agents"
    authors: []
    year: 2025
    url: "https://arxiv.org/abs/2512.01822"
    abstract: "Benchmarking framework for evaluating AI agent innovation potential. Relevant to agentic system evaluation."
    source: arxiv
    priority: 6
    subtopics: ["agentic-evaluation", "automated-metrics"]
    read: false
    notes_file: null
    added: "2025-12-17"

  # === TO EXPLORE (Mentioned Resources) ===

  - key: openai_evals_framework
    title: "OpenAI Evals Framework"
    authors: ["OpenAI"]
    year: 2023
    url: "https://github.com/openai/evals"
    abstract: "Open-source evaluation framework. Supports basic evals (exact/fuzzy match), model-graded evals, and custom evaluation logic. Includes YAML/JSON-based eval creation without code. Model-graded templates: fact, closedqa, battle."
    source: web
    priority: 8
    subtopics: ["automated-metrics", "evaluation-frameworks"]
    read: true
    notes_file: "Claude-Notes/Paper-Summaries/openai_evals_framework.md"
    added: "2025-12-17"

  - key: liu_6ragevals
    title: "There Are Only 6 RAG Evals"
    authors: ["Liu, Jason"]
    year: 2025
    url: "https://jxnl.co/writing/2025/05/19/there-are-only-6-rag-evals/"
    abstract: "Simplified RAG evaluation framework using 6 conditional relationships between Q (Question), C (Context), A (Answer). Three-tier structure: Tier 1 (IR metrics), Tier 2 (primary RAG), Tier 3 (advanced). Key insight: debug retrieval first, then generation."
    source: blog
    priority: 7
    subtopics: ["automated-metrics", "evaluation-frameworks"]
    read: true
    notes_file: "Claude-Notes/Paper-Summaries/liu_6ragevals.md"
    added: "2025-12-17"

  # === USER-PROVIDED (ITEA) ===

  - key: itea2025llmtesting
    title: "Advancing the Test Science of LLM-enabled Systems: A Survey of Factors and Conditions that Matter Most"
    authors: ["O'Brien, Karen"]
    year: 2025
    url: "https://itea.org/journals/volume-46-3/advancing-the-test-science-of-llm-enabled-systems/"
    abstract: "ITEA Journal article on scientific operational testing of LLM-enabled systems for DoD applications. Emphasizes architecture diagrams for identifying evaluation factors, discusses LLM-as-Judge limitations for complex cognitive tasks (68%/64% agreement rates with SMEs), and recommends human SME scoring for nuanced evaluation."
    source: journal
    priority: 8
    subtopics: ["evaluation-frameworks", "llm-as-judge", "practical-guidance"]
    read: true
    notes_file: "Claude-Notes/Paper-Summaries/itea2025llmtesting.md"
    added: "2025-12-17"
