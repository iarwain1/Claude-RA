# Reading Queue for LLM Prompt Evaluation
# Papers queued for detailed reading

review_name: "LLM Prompt Evaluation"
last_updated: "2025-12-17"

queue:
  # High priority - foundational practitioner guides
  - key: husain2024evals
    priority: 10
    reason: "Core foundational guide on LLM evals with 3-level framework. Most cited resource on topic."
    added: "2025-12-17"
    status: done

  - key: husain2024llmjudge
    priority: 10
    reason: "Comprehensive guide on LLM-as-judge - key evaluation methodology. Based on 30+ company implementations."
    added: "2025-12-17"
    status: done

  - key: husain2025evalsfaq
    priority: 10
    reason: "FAQ covers agentic evaluation, RAG, multi-step workflows - directly relevant to review scope."
    added: "2025-12-17"
    status: done

  - key: husain2025fieldguide
    priority: 9
    reason: "Practical guidance on avoiding evaluation pitfalls. Based on production implementations."
    added: "2025-12-17"
    status: done

  # Survey papers
  - key: guo2025agentbenchmark
    priority: 9
    reason: "Recent comprehensive survey (150+ papers) on LLM benchmarks and solutions taxonomy."
    added: "2025-12-17"
    status: done

  # Anthropic resources
  - key: anthropic2025evaltool
    priority: 8
    reason: "Official Anthropic evaluation tooling documentation - practical implementation reference."
    added: "2025-12-17"
    status: done

  - key: anthropic2025toolsforagents
    priority: 8
    reason: "Agent/tool evaluation best practices from major LLM provider."
    added: "2025-12-17"
    status: done

  # Evaluation frameworks
  - key: openai_evals_framework
    priority: 8
    reason: "Standard open-source evaluation framework. Widely used in industry."
    added: "2025-12-17"
    status: done

  # Context/prompt optimization
  - key: ace2025contextengineering
    priority: 7
    reason: "Novel framework for evolving prompts/contexts. Addresses key problems in iterative prompting."
    added: "2025-12-17"
    status: done

  - key: survey2025codeintelligence
    priority: 7
    reason: "Comprehensive survey covering prompting paradigms and evaluation in code domain."
    added: "2025-12-17"
    status: done

  # Additional Anthropic resources
  - key: anthropic2025promptengineering
    priority: 7
    reason: "Official prompt engineering overview with evaluation guidance."
    added: "2025-12-17"
    status: done

  - key: liu_6ragevals
    priority: 7
    reason: "Simplified RAG evaluation framework - 6 conditional relationships, 3-tier structure."
    added: "2025-12-17"
    status: done

  # Major survey - now verified
  - key: mei2025contextengineering
    priority: 9
    reason: "Massive survey (1,400+ papers) on context engineering. Covers RAG, memory, tool reasoning, multi-agent systems."
    added: "2025-12-17"
    status: done

  # Additional verified papers
  - key: anam2025prompteffectiveness
    priority: 7
    reason: "Empirical study (243 respondents) on prompt structure vs. effectiveness. Human evaluation data."
    added: "2025-12-17"
    status: done

  - key: contexteng2025evolution
    priority: 6
    reason: "Historical perspective on context engineering evolution. Good background context."
    added: "2025-12-17"

  # Now verified
  - key: prompting2025practice
    priority: 8
    reason: "Empirical study of real developer prompting behavior. High relevance for practical evaluation."
    added: "2025-12-17"
    status: done

  - key: promptcot2025scaling
    priority: 7
    reason: "Prompt synthesis scaling for reasoning. Relevant to automated prompt optimization."
    added: "2025-12-17"
    status: done

  - key: innogym2025benchmark
    priority: 6
    reason: "Agent benchmarking framework. Relevant to agentic evaluation subtopic."
    added: "2025-12-17"

  # User-provided (ITEA)
  - key: itea2025llmtesting
    priority: 8
    reason: "DoD/operational testing perspective on LLM evaluation. Discusses LLM-as-Judge limitations."
    added: "2025-12-17"
    status: done
