# Appendix A, Section 2.4: Qualitative Research Methods

## Overview

Qualitative research methods aim to understand phenomena through rich description, interpretation, and attention to context and meaning. Where quantitative methods measure and count, qualitative methods describe and interpret. Where quantitative methods ask "how much," qualitative methods ask "how" and "why."

For AI evaluation, qualitative methods are essential complements to quantitative approaches. Numbers can tell us that AI improves productivity by 15%, but they can't tell us how users experience AI, what strategies work, where frustrations arise, or why some users succeed while others struggle. These questions require qualitative investigation.

The "vibes" problem in AI evaluation—knowing good output when you see it but struggling to articulate criteria—is fundamentally a qualitative challenge. Understanding what quality means, how experts recognize it, and why some AI outputs feel right while others don't requires qualitative inquiry.

---

## Core Concepts

### Ethnography and Participant Observation

Ethnography involves extended immersion in a setting to understand practice from participants' perspectives. Rather than brief surveys or experiments, ethnographers spend significant time observing, participating, and building deep understanding.

For AI, ethnographic approaches might involve researchers embedded in teams using AI, observing how AI integrates into workflows, what problems arise, and how practices evolve. This reveals things surveys miss: informal workarounds, unspoken norms, emergent practices.

### Interviews

Interviews range from structured (fixed questions, standardized for comparison) to unstructured (open-ended exploration following participant's lead):

**Structured interviews** ensure comparable data across participants but may miss unexpected insights.

**Semi-structured interviews** cover key topics while allowing flexibility to pursue emergent themes.

**Unstructured interviews** maximize depth and participant voice but make comparison difficult.

**Focus groups** leverage group dynamics—participants respond to each other, surfacing shared understandings and disagreements.

For AI evaluation, interviews can explore user experience, perceived value, frustrations, and strategies. They're particularly valuable for understanding heterogeneity: why does AI work better for some users than others?

### Grounded Theory

Grounded theory builds theory inductively from data rather than testing pre-specified hypotheses. Researchers collect data, code it for themes, compare cases, and iteratively develop theoretical understanding.

For emerging phenomena like AI use, grounded theory is valuable: we don't know enough to specify hypotheses in advance. Grounded theory develops frameworks from what's observed rather than imposing external categories.

Key processes include:
- **Open coding**: Initial categorization of data
- **Axial coding**: Relating categories to each other
- **Selective coding**: Integrating around core categories
- **Theoretical sampling**: Purposefully selecting cases to develop theory
- **Constant comparison**: Continuously comparing new data to emerging theory

### Case Study Methodology

Case studies investigate bounded cases in depth using multiple data sources. A case study of AI adoption in a particular organization might draw on interviews, documents, observations, and quantitative data to build comprehensive understanding.

Case studies support analytic generalization—generalization to theory rather than to populations. The goal isn't claiming that findings apply to all organizations, but developing theoretical insights that might transfer.

Types of case studies include:
- **Exploratory**: Initial investigation of phenomena
- **Descriptive**: Rich description of cases
- **Explanatory**: Explaining how and why
- **Multiple-case**: Comparing across cases for pattern

### Process Tracing

Process tracing examines causal mechanisms—not just whether X affects Y, but how. By tracing the sequence of events, decisions, and mechanisms linking cause to effect, process tracing provides within-case causal inference.

For AI, process tracing might examine how AI adoption leads (or fails to lead) to productivity gains: What sequence of changes occurred? What mechanisms operated? Where did the chain break?

### Validity in Qualitative Research

Qualitative research has different validity concepts:

**Credibility** parallels internal validity—are findings credible given the data? Techniques include prolonged engagement, triangulation (multiple sources), and member checking (verifying interpretations with participants).

**Transferability** parallels external validity—can findings transfer to other contexts? Thick description enables readers to judge transferability to their contexts.

**Dependability** parallels reliability—would the study produce similar findings if repeated? Audit trails document procedures for scrutiny.

**Confirmability** addresses researcher bias—are findings grounded in data rather than researcher preconceptions? Reflexivity acknowledges researcher positioning.

---

## What Transfers to AI Evaluation

**Understanding "why"** is qualitative methods' core contribution. When AI helps some users but not others, qualitative inquiry can reveal why—what strategies successful users employ, what barriers others face.

**Exploratory power** is essential when we don't know what to measure. Qualitative methods can identify important dimensions before quantitative measurement.

**Context sensitivity** captures how AI use is embedded in work practices, relationships, and organizational systems. Decontextualized metrics may miss these crucial factors.

**User perspective** understands AI from the user's point of view—how they experience AI, what it means to them, what frustrations and satisfactions they encounter.

**Rich description** enables others to understand AI use in depth and judge whether findings apply to their contexts.

---

## What Breaks for Generative AI

**Generalization concerns** arise: qualitative findings from one setting may not transfer to others. Sample sizes are small. Representativeness isn't the goal.

**Resource intensity** limits scale. Deep qualitative work requires time and expertise. It can't be applied to thousands of cases.

**Credibility for some audiences** is limited. Stakeholders accustomed to quantitative evidence may discount qualitative findings. This is often a mistake—qualitative evidence provides different but equally important insights.

**Rapid change** challenges qualitative work that requires extended engagement. By the time findings are developed, the context may have shifted.

---

## What Can Be Adapted

**Rapid qualitative assessment** provides faster, lighter-touch qualitative investigation. While sacrificing some depth, rapid methods can inform decision-making on shorter timescales.

**Mixed methods** combine qualitative and quantitative approaches. Qualitative methods can inform quantitative measures, explain quantitative findings, or provide complementary evidence.

**Participatory approaches** involve stakeholders in research design and interpretation, building relevance and buy-in while generating insights.

---

## Implications for AI Evaluation

**Use qualitative methods to understand how and why.** Numbers tell you AI helps by 15%; qualitative methods tell you how users experience this, what makes it work, where it fails.

**Employ qualitative methods when you don't know what to measure.** Early exploration should be qualitative before settling on metrics.

**Observe AI use in practice.** Surveys and experiments miss what observation catches—workarounds, informal practices, integration into complex workflows.

**Combine with quantitative methods.** Mixed methods provide comprehensive understanding: quantitative for how much, qualitative for how and why.

**Attend to user experience.** AI evaluation should include the user perspective—not just whether AI produces good outputs, but how users experience working with AI.

**Use qualitative methods to develop theory.** We're still developing frameworks for understanding AI's effects. Grounded theory approaches can build understanding inductively.

---

## Key References

- **Creswell, J.W., & Poth, C.N. (2018). *Qualitative Inquiry and Research Design* (4th ed.).** Comprehensive introduction to qualitative traditions.

- **Yin, R.K. (2018). *Case Study Research and Applications* (6th ed.).** Definitive guide to case study methodology.

- **Miles, M.B., Huberman, A.M., & Saldaña, J. (2014). *Qualitative Data Analysis* (3rd ed.).** Practical guide to analyzing qualitative data.

- **Charmaz, K. (2014). *Constructing Grounded Theory* (2nd ed.).** Modern approach to grounded theory.

- **Suchman, L. (1987). *Plans and Situated Actions*.** Classic study of human-computer interaction in context.

- **Beach, D., & Pedersen, R.B. (2019). *Process-Tracing Methods* (2nd ed.).** Comprehensive treatment of process tracing.

---

## Connections to Other Sections

Qualitative methods connect to several other disciplines covered in this appendix:

- **Section 2.1 (Experimental Design)** provides complementary quantitative methods; mixed methods combine both traditions.

- **Section 5.1 (HCI)** shares concern for understanding technology use in context, with related methods like contextual inquiry.

- **Section 5.3 (Organizational Behavior)** applies qualitative methods to organizational phenomena including technology adoption.

- **Section 6.2 (Program Evaluation)** incorporates qualitative methods for process evaluation and developmental evaluation.

- **Section 2.5 (Survey Methodology)** provides another data collection approach that can be combined with qualitative methods.

---

*[End of Section 2.4]*
