# Appendix A, Section 6.2: Program Evaluation

## Overview

Program evaluation is the systematic assessment of programs and interventions—typically social programs, educational initiatives, or policy interventions. For AI, program evaluation provides frameworks for evaluating AI as an organizational intervention: a change implemented with the goal of producing some benefit.

Program evaluation asks not just "does the technology work?" but "does the program of implementing this technology achieve its goals?" This broader frame is essential for AI evaluation that goes beyond technical capability to organizational impact.

---

## Core Concepts

### Theory of Change / Logic Models

Logic models make causal assumptions explicit:

**Inputs**: Resources invested (AI tools, training, time, infrastructure)

**Activities**: What happens (AI use, workflow changes, user training)

**Outputs**: Direct products (documents produced, queries answered, tasks completed)

**Outcomes**: Short-term changes (time saved, quality improved, errors reduced)

**Impact**: Long-term effects (organizational performance, competitive position, mission success)

The arrows between stages are assumptions. We assume inputs lead to activities lead to outputs lead to outcomes lead to impact. Each arrow can be tested; each is where the chain can break.

For AI, logic models force explicit thinking:
- How is AI supposed to create value?
- What must happen for inputs to produce impacts?
- Where might the chain break?

A logic model for AI might be: Provide AI tools (input) → Users adopt AI (activity) → More documents produced (output) → Time saved (outcome) → Better organizational performance (impact). Each arrow is contestable.

### Formative vs. Summative Evaluation

**Formative evaluation** aims to improve the program during implementation:
- What's working? What isn't?
- How can we do better?
- Ongoing, adaptive, learning-focused
- Feedback for improvement

**Summative evaluation** aims to judge overall worth or effectiveness:
- Did the program work?
- Should it continue?
- Endpoint assessment for accountability
- Judgment about program value

For AI, formative evaluation is often more valuable:
- AI use is evolving rapidly
- Learning how to use AI better is as important as judging current use
- Flexibility to adapt is essential

Don't wait until the end to evaluate; evaluate continuously and use findings to improve.

### Process vs. Outcome Evaluation

**Process evaluation** asks: Was the intervention implemented as intended?
- Did people actually use AI?
- How did they use it?
- Were training and support provided?
- What was the quality of implementation?

**Outcome evaluation** asks: Did desired effects occur?
- Did productivity improve?
- Did quality increase?
- Did costs decrease?

Process evaluation is essential for interpreting outcomes:
- If AI "doesn't work," is that because AI is ineffective or because it wasn't actually used (or wasn't used well)?
- Negative outcome evaluation with poor implementation doesn't condemn the intervention

### Implementation Fidelity

Implementation fidelity assesses whether the intervention was delivered as designed:

**Adherence**: Was the core intervention delivered? Was AI actually provided and used?

**Dosage**: How much exposure did participants have? How often was AI used?

**Quality**: How well was the intervention delivered? Was AI use effective?

**Responsiveness**: How did participants respond? Were they engaged? Resistant?

For AI, implementation fidelity asks:
- Did people actually use AI? How often? How much?
- How well did they use it? Effective prompting? Appropriate verification?
- Were they engaged? Or going through the motions?

Low fidelity—people not actually using AI, or using it poorly—explains many null findings. Don't conclude AI doesn't work if AI wasn't really used.

### Developmental Evaluation

For innovative initiatives in complex environments, developmental evaluation embeds evaluators as part of the development team:

**Ongoing**: Evaluation is continuous, not periodic
**Adaptive**: Methods adapt as understanding grows
**Emergent**: Outcomes and paths aren't pre-specified
**Learning-focused**: Goal is learning, not just judgment

For AI—where we're still learning what effective AI use looks like—developmental evaluation may be more appropriate than fixed evaluation designs. We don't know enough to specify outcomes in advance.

### Realist Evaluation

Realist evaluation asks: "What works, for whom, in what circumstances, and why?"

**Context**: Conditions that affect whether intervention works
**Mechanism**: How the intervention produces effects
**Outcome**: What effects are produced

**Context-Mechanism-Outcome (CMO) configurations** explain variation. AI might work for some users in some contexts through some mechanisms but not others.

Realist evaluation moves beyond "does AI work?" to "for whom, when, and how?"

---

## What Transfers to AI Evaluation

**Logic model discipline**: Make explicit how AI is supposed to create value. What's the theory of change?

**Implementation fidelity**: Check whether AI is actually being used and how. Don't evaluate an intervention that didn't happen.

**Formative evaluation**: Focus on learning and improvement, not just judgment. Use findings to improve.

**Developmental evaluation**: Appropriate for emerging, evolving AI use where we're still learning.

**Realist perspective**: Understand what works for whom, when, and why. Don't seek universal answers when context matters.

---

## What Breaks for Generative AI

**Pre-specified outcomes**: Traditional evaluation pre-specifies outcomes. AI outcomes may be emergent and unexpected.

**Stable intervention**: Traditional evaluation assumes stable intervention. AI keeps changing.

**Clear comparison**: Experimental designs assume clear treatment/control distinction. "Using AI" is heterogeneous.

**Long evaluation timelines**: Traditional evaluation takes time. AI changes faster than typical evaluation cycles.

---

## What Can Be Adapted

**Rapid evaluation cycles** that provide faster feedback. Shorter cycles, more frequent assessment.

**Adaptive evaluation designs** that evolve as AI use evolves.

**Mixed methods** combining quantitative outcomes with qualitative process understanding.

**Learning-oriented evaluation** that prioritizes learning over judgment.

---

## Implications for AI Evaluation

**Develop logic models** for how AI is supposed to create value. What's the theory of change? Where might it break?

**Check implementation fidelity**: Is AI actually being used? How? How well? Don't evaluate ghost interventions.

**Use formative evaluation** to improve AI use, not just judge it. Continuous learning beats endpoint judgment.

**Apply developmental evaluation** for new AI deployments. We're still learning; evaluation should support learning.

**Seek realist understanding** of variation in AI effects. For whom? When? Why?

---

## Key References

- **Patton, M.Q. (2010). *Developmental Evaluation*.** Evaluation for innovative initiatives.

- **Patton, M.Q. (2008). *Utilization-Focused Evaluation* (4th ed.).** Evaluation designed to be useful.

- **Pawson, R., & Tilley, N. (1997). *Realistic Evaluation*.** Realist evaluation framework.

- **Rossi, P.H., Lipsey, M.W., & Henry, G.T. (2018). *Evaluation: A Systematic Approach* (8th ed.).** Comprehensive program evaluation text.

- **Fixsen, D.L., et al. (2005). *Implementation Research: A Synthesis of the Literature*.** Implementation science foundations.

---

## Connections to Other Sections

Program evaluation connects to several other disciplines covered in this appendix:

- **Section 2.1 (Experimental Design)** provides methods for outcome evaluation.

- **Section 2.4 (Qualitative Methods)** provides methods for process evaluation and understanding.

- **Section 5.3 (Organizational Behavior)** addresses implementation and organizational factors.

- **Section 3.1 (IS Economics)** addresses IT implementation and complementary investments.

- **Section 6.4 (Clinical Trials)** provides related phased evaluation approaches.

---

*[End of Section 6.2]*
