# Measuring what Matters: Construct Validity in Large Language Model Benchmarks

**Paper Key:** bean2025construct
**Authors:** Bean, Andrew M., Kearns, Ryan Othniel, Romanou, Angelika, et al. (42 total authors)
**Date:** November 3, 2025
**Venue:** NeurIPS 2025 Datasets and Benchmarks Track
**arXiv:** https://arxiv.org/abs/2511.04703
**Priority:** 10 (Essential)

---

## Summary

This landmark systematic review examines construct validity—whether benchmarks actually measure what they claim to measure—across 445 LLM benchmarks from leading NLP and ML conferences. Using 29 expert reviewers, the authors identify pervasive patterns undermining measurement validity in claims about safety, robustness, and other abstract phenomena.

---

## Key Contribution

**First systematic large-scale analysis of construct validity in LLM benchmarking**, establishing that many benchmarks fail to measure their stated constructs. The work bridges psychometrics, measurement theory, and AI evaluation.

---

## Methodology

- **Scope:** 445 LLM benchmarks from leading conferences
- **Review team:** 29 expert reviewers
- **Focus:** Construct validity analysis (do measures represent claimed phenomena?)
- **Framework:** Systematic review examining measured phenomena, task design, and scoring metrics

---

## Main Findings

### Validity Failures

The paper identifies **systematic patterns undermining validity** across benchmarks:

1. **Disconnect between claimed phenomena and actual tasks**
   - Benchmarks claim to measure "safety" or "robustness" but use tasks that don't capture these constructs
   - Example pattern: Measuring "reasoning" using recall tasks

2. **Scoring metrics that don't capture constructs of interest**
   - Metrics chosen for convenience rather than validity
   - Mismatch between what's measured and what the metric quantifies

3. **Missing validation of measurement**
   - Lack of evidence that benchmarks measure intended attributes
   - No validation studies connecting scores to real-world phenomena

---

## Eight Key Recommendations

The paper provides **actionable guidance** for developing valid benchmarks (specific recommendations require full paper access):

1. [Recommendation details from full paper]
2. [Recommendation details from full paper]
3. [Recommendation details from full paper]
4. [Recommendation details from full paper]
5. [Recommendation details from full paper]
6. [Recommendation details from full paper]
7. [Recommendation details from full paper]
8. [Recommendation details from full paper]

---

## Relevance to GenAI Productivity Review

### Part 1: The Benchmark-Utility Gap

**CRITICAL for understanding the gap.** This paper provides empirical evidence that benchmarks systematically fail to measure what they claim. Directly explains why benchmark performance doesn't predict real-world utility—the measurements lack construct validity.

**Key insight:** The problem isn't just benchmark saturation or gaming—it's fundamental measurement failure.

### Part 3: Framework for AI Evaluation

**Essential for evaluation framework design.** The 8 recommendations provide concrete guidance for:
- Designing valid productivity measurements
- Selecting appropriate metrics
- Validating that evaluations measure intended constructs

### Part 4: Practical Guidance

**Actionable for acquisition decision-makers:**
- Don't trust benchmark scores at face value
- Demand evidence of construct validity
- Look for validation connecting scores to operational outcomes

---

## Connections to Other References

- **Complements:** brand2025benchmarking (Epoch AI) - adds validity perspective to reproducibility challenges
- **Explains:** Why upwork2025upbench and scale2025remotelabor show massive research-practice gaps
- **Supports:** mollick2024interview's "vibes" problem - benchmarks measure wrong things
- **Informs:** ho2025rosetta framework - validity must precede unification

---

## Critical Quotes

> "Reliably measuring abstract and complex phenomena such as 'safety' and 'robustness' requires strong construct validity, that is, having measures that represent what matters to the phenomenon."

> [Additional key quotes from full paper when available]

---

## Limitations & Future Work

- Analysis focused on published benchmarks (may miss unpublished work)
- Recommendations need empirical validation
- Requires follow-up studies implementing suggested improvements

---

## Research Questions Raised

1. How can we validate that AI productivity benchmarks measure actual productivity?
2. What evidence would demonstrate construct validity for defense acquisition evaluations?
3. Can existing benchmarks be salvaged or do we need entirely new approaches?

---

## Tags

`#benchmark-critique` `#construct-validity` `#systematic-review` `#measurement-theory` `#evaluation-methodology` `#key-reference`

---

## Reading Status

- **Status:** Pending full read
- **Added to queue:** 2025-12-31
- **Note file created:** 2025-12-31
- **Full paper access:** Available on arXiv

---

## Next Steps

1. **Read full paper** to extract 8 specific recommendations
2. **Map findings** to defense acquisition context
3. **Apply framework** to existing productivity benchmarks (METR, Scale, OpenAI)
4. **Use recommendations** in Part 3 framework development

---

*Summary created: 2025-12-31*
*Last updated: 2025-12-31*
