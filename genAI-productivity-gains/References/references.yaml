# Literature Review References
# Topic: Measuring Productivity Gains from Generative AI

metadata:
  review_name: "genAI-productivity-gains"
  topic: "Evaluating AI Productivity for Defense Acquisition: From Benchmarks to Operational Effectiveness"
  created: "2025-12-10"
  last_updated: "2025-12-31"
  primary_audience: "Military acquisition decision-makers"
  secondary_audience: "Testing & Evaluation (T&E) experts for military systems"
  total_references: 91
  recent_additions: "11 papers added Dec 31, 2025: 8 high-priority (benchmarks/agents/productivity) + 3 medium-priority (safety/alignment)"

references:
  #############################################################################
  # TIER 1: CRITICAL EMPIRICAL STUDIES (Score 9-10)
  #############################################################################

  - key: metr2025developer
    title: "Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity"
    authors: ["METR"]
    year: 2025
    month: 7
    url: "https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/"
    arxiv: "https://arxiv.org/abs/2507.09089"
    abstract: |
      RCT with 16 experienced open-source developers, 246 tasks. KEY FINDING: AI tools made
      developers 19% SLOWER, despite developers expecting 24% speedup and believing (even after)
      they got 20% speedup. 3/4 participants saw reduced performance. Developers reported AI
      behaved like inexperienced team member, spent 9% of time reviewing AI outputs + 4% waiting.
      Tools: primarily Cursor Pro with Claude 3.5/3.7 Sonnet.
    source: research_org
    priority: 10
    subtopics: ["productivity-studies", "human-ai-teaming", "evaluation-methodology"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["key-reference", "empirical-anchor", "RCT", "negative-finding"]
    key_findings:
      - "19% slowdown with AI tools (contradicts expectations)"
      - "Developers believed they got 20% speedup despite actual slowdown"
      - "75% of participants saw reduced performance"
      - "Expert forecasters predicted 38-39% speedup"

  - key: scale2025remotelabor
    title: "Remote Labor Index: Measuring AI Automation of Remote Work"
    authors: ["Scale AI", "Center for AI Safety (CAIS)"]
    year: 2025
    month: 10
    url: "https://www.remotelabor.ai/"
    arxiv: "https://arxiv.org/abs/2510.26787"
    pdf: "https://static.scale.com/uploads/654197dc94d34f66c0f5184e/Remote_Labor_Index%20(4).pdf"
    abstract: |
      First benchmark testing AI agents on paid freelance work (240 projects, 23 categories,
      6,000+ hours, $140K+). KEY FINDING: Best agent (Manus) achieved only 2.5% automation rate.
      97.5% failure rate shows AI not capable of autonomously performing complex professional work.
      Failures: 45.6% quality issues, 35.7% incomplete/malformed deliverables, 17.6% technical issues.
      Success areas: audio tasks, image generation, report writing, data retrieval.
    source: research_org
    priority: 10
    subtopics: ["productivity-studies", "agent-evaluation", "benchmarks"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["key-reference", "empirical-anchor", "benchmark-utility-gap"]
    key_findings:
      - "2.5% automation rate (highest performing agent)"
      - "97.5% failure rate on professional work"
      - "Median project: 11.5 hours human time, $200 value"
      - "AI succeeds on generative tasks, fails on complex editing/multi-step specs"

  - key: openai2025gdpval
    title: "GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"
    authors: ["OpenAI"]
    year: 2025
    month: 9
    url: "https://openai.com/index/gdpval/"
    arxiv: "https://arxiv.org/abs/2510.04374"
    pdf: "https://cdn.openai.com/pdf/d5eb7428-c4e9-4a33-bd86-86dd4bcf12ce/GDPval.pdf"
    abstract: |
      Evaluation suite covering 9 sectors, 44 occupations ($3T annually). Tasks created by
      professionals averaging 14 years experience. Uses blind comparison methodology.
      KEY FINDINGS: Frontier models 100x faster and 100x cheaper than industry experts.
      Human-assisted workflows 1.4x faster, 1.6x cheaper. Performance doubled from GPT-4o to GPT-5.
      Claude Opus 4.1 best on aesthetics; GPT-5 best on accuracy.
    source: ai_lab
    priority: 10
    subtopics: ["productivity-studies", "evaluation-methodology", "benchmarks"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["key-reference", "methodology-example"]
    key_findings:
      - "100x faster and 100x cheaper than human experts"
      - "Human+AI workflows 1.4x faster, 1.6x cheaper"
      - "Humans averaged 6-7 hours ($361) per task"
      - "Performance doubled GPT-4o → GPT-5"

  - key: upwork2025upbench
    title: "UpBench: A Dynamically Evolving Real-World Benchmark for AI Agents"
    authors: ["Upwork"]
    year: 2025
    month: 11
    url: "https://www.upwork.com/static/webflow/assets/webflow-human-agent-productivity-index/upbench_paper.pdf"
    related_url: "https://investors.upwork.com/news-releases/news-release-details/upwork-humanagent-productivity-index-reveals-70-boost-work"
    abstract: |
      Real-world benchmark using paid Upwork projects. Human+Agent Productivity Index (HAPI)
      findings: Human+agent collaboration increased project completion by up to 70% vs agents alone.
      Claude Sonnet 4: 64%→93% completion with human feedback (data science). GPT-5: 30%→50%
      (engineering). Web dev highest standalone: 68%. Expert freelancers (100% satisfaction,
      top-rated, $1M+ cumulative earnings) evaluate outputs.
    source: industry
    priority: 9
    subtopics: ["productivity-studies", "human-ai-teaming", "agent-evaluation"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["key-reference", "real-world-data"]
    key_findings:
      - "70% boost in completion with human+agent vs agent alone"
      - "Human feedback transforms agent performance (64%→93%)"
      - "Orders of magnitude time savings vs human alone"
      - "Per-criterion expert feedback enables fine-grained analysis"

  - key: anthropic2025economic
    title: "The Anthropic Economic Index"
    authors: ["Anthropic"]
    year: 2025
    url: "https://www.anthropic.com/news/the-anthropic-economic-index"
    related_url: "https://www.anthropic.com/research/anthropic-economic-index-september-2025-report"
    abstract: |
      Analysis of millions of anonymized Claude.ai conversations revealing AI integration into
      real-world tasks. KEY FINDINGS: 57% augmentation vs 43% automation. 36% of occupations
      use AI in 25%+ of tasks; 4% use it in 75%+ of tasks. Enterprise API shows 77% automation
      rate vs consumer's 50%. Software dev and technical writing dominate usage. Open-sourced
      dataset for researchers.
    source: ai_lab
    priority: 10
    subtopics: ["productivity-studies", "economic-impact"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["key-reference", "usage-data"]
    key_findings:
      - "57% augmentation vs 43% automation"
      - "Enterprise API: 77% automation rate"
      - "Directive automation grew from 27% to 39% in 8 months"
      - "Educational instruction tasks grew 40%"

  - key: anthropic2025productivity
    title: "Estimating AI productivity gains from Claude conversations"
    authors: ["Anthropic"]
    year: 2025
    url: "https://assets.anthropic.com/m/28fda2ad148e2bf5/original/Estimating-AI-productivity-gains-from-Claude-conversations.pdf"
    abstract: |
      Uses Claude to estimate task completion times from 100K real conversations. 80% estimated
      time savings. Validated against JIRA tickets. Acknowledges limitations of self-report and
      selection bias.
    source: ai_lab
    priority: 9
    subtopics: ["productivity-studies", "evaluation-methodology"]
    read: true
    notes_file: null
    added: "2025-12-10"
    tags: ["key-reference", "methodology-example"]

  #############################################################################
  # TIER 1: KEY CONCEPTUAL/FRAMEWORK PAPERS
  #############################################################################

  - key: toner2024jaggedness
    title: "Taking Jaggedness Seriously"
    authors: ["Toner, Helen"]
    year: 2024
    url: "https://helentoner.substack.com/p/taking-jaggedness-seriously"
    abstract: |
      Conceptual analysis of the "jagged capability frontier" (term from Mollick/Karpathy).
      Tasks that seem similarly difficult to humans vary wildly in AI capability - some easy,
      some borderline, some impossible. KEY IMPLICATIONS: 1) Search space of possible futures
      explodes, 2) AI-for-good matters more, 3) Centaurs (human-AI combinations) may persist
      meaningfully. Many expect jaggedness to decrease but this may not happen.
    source: blog
    priority: 9
    subtopics: ["jagged-frontier", "evaluation-methodology"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["key-reference", "conceptual-framework"]
    key_findings:
      - "Jagged frontier means similar-seeming tasks have wildly different AI capabilities"
      - "Jaggedness may persist, not necessarily decrease over time"
      - "Centaur (human+AI) combinations may be meaningful long-term"

  - key: mollick2024interview
    title: "Giving Your AI a Job Interview"
    authors: ["Mollick, Ethan"]
    year: 2024
    url: "https://www.oneusefulthing.org/p/giving-your-ai-a-job-interview"
    abstract: |
      Discussion of benchmark limitations; 'vibes' matter but aren't rigorous; organizations
      need to 'interview' AI for their specific context. Introduces practical evaluation
      approaches for organizations.
    source: blog
    priority: 9
    subtopics: ["evaluation-methodology", "benchmarks"]
    read: true
    notes_file: null
    added: "2025-12-10"
    tags: ["key-reference", "vibes-problem"]

  #############################################################################
  # TIER 1: EVALUATION METHODOLOGY SOURCES
  #############################################################################

  - key: metr2025timehorizons
    title: "Research Update: Towards Reconciling Slowdown with Time Horizons"
    authors: ["METR"]
    year: 2025
    month: 8
    url: "https://metr.org/blog/2025-08-12-research-update-towards-reconciling-slowdown-with-time-horizons/"
    abstract: |
      METR research update on AI capability slowdown vs. time horizons for task completion.
      Analyzes how AI performance varies with task duration and complexity.
    source: research_org
    priority: 9
    subtopics: ["evaluation-methodology", "agent-evaluation"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["methodology"]

  - key: metr2025domains
    title: "How Does Time Horizon Vary Across Domains?"
    authors: ["METR"]
    year: 2025
    month: 7
    url: "https://metr.org/blog/2025-07-14-how-does-time-horizon-vary-across-domains/"
    abstract: |
      Analysis of how AI time horizons (how long AI can work autonomously) vary across
      different domains and task types.
    source: research_org
    priority: 8
    subtopics: ["evaluation-methodology", "agent-evaluation"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["methodology"]

  - key: aisi2024claude35
    title: "Pre-deployment evaluation of Anthropic's upgraded Claude 3.5 Sonnet"
    authors: ["UK AISI", "US AISI"]
    year: 2024
    month: 11
    url: "https://www.aisi.gov.uk/blog/pre-deployment-evaluation-of-anthropics-upgraded-claude-3-5-sonnet"
    related_url: "https://www.nist.gov/news-events/news/2024/11/pre-deployment-evaluation-anthropics-upgraded-claude-35-sonnet"
    abstract: |
      First joint US-UK pre-deployment evaluation of frontier AI. Tested biological capabilities,
      cyber capabilities, software/AI development, safeguard efficacy. Found safeguards can be
      routinely circumvented. Model solved 36% of cybersecurity apprentice-level challenges.
    source: government
    priority: 8
    subtopics: ["evaluation-methodology", "risk-management", "standards-frameworks"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["government-eval", "pre-deployment"]

  #############################################################################
  # TIER 2: IMPORTANT SUPPORTING SOURCES
  #############################################################################

  - key: epoch2025benchmarks
    title: "AI Benchmarking Dashboard"
    authors: ["Epoch AI"]
    year: 2025
    url: "https://epoch.ai/benchmarks"
    abstract: "Comprehensive tracking of AI benchmark performance over time."
    source: research_org
    priority: 8
    subtopics: ["benchmarks", "evaluation-methodology"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["dashboard", "tracking"]

  - key: epoch2025economic
    title: "The Real Reason AI Benchmarks Haven't Reflected Economic Impacts"
    authors: ["Epoch AI"]
    year: 2025
    url: "https://epoch.ai/gradient-updates/the-real-reason-ai-benchmarks-havent-reflected-economic-impacts"
    abstract: "Analysis of why benchmark improvements haven't translated to economic productivity gains."
    source: research_org
    priority: 9
    subtopics: ["benchmarks", "economic-impact"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["benchmark-utility-gap"]

  - key: epoch2025swebench
    title: "What Skills Does SWE-Bench Verified Evaluate?"
    authors: ["Epoch AI"]
    year: 2025
    url: "https://epoch.ai/blog/what-skills-does-swe-bench-verified-evaluate"
    abstract: "Detailed analysis of what the popular SWE-Bench coding benchmark actually measures."
    source: research_org
    priority: 7
    subtopics: ["benchmarks", "evaluation-methodology"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["benchmark-analysis"]

  - key: metr2025rd
    title: "Evaluating R&D Capabilities of LLMs"
    authors: ["METR"]
    year: 2024
    month: 11
    url: "https://metr.org/blog/2024-11-22-evaluating-r-d-capabilities-of-llms/"
    abstract: "Framework for evaluating AI R&D capabilities, relevant to understanding AI's potential for accelerating AI development."
    source: research_org
    priority: 8
    subtopics: ["evaluation-methodology", "agent-evaluation"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["methodology", "ai-rd"]

  - key: metr2025kernel
    title: "Measuring Automated Kernel Engineering"
    authors: ["METR"]
    year: 2025
    month: 2
    url: "https://metr.org/blog/2025-02-14-measuring-automated-kernel-engineering/"
    abstract: "Evaluation of AI capabilities in kernel engineering tasks."
    source: research_org
    priority: 7
    subtopics: ["evaluation-methodology", "productivity-studies"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["methodology"]

  - key: anthropic2025statistical
    title: "A Statistical Approach to Model Evaluations"
    authors: ["Anthropic"]
    year: 2025
    url: "https://www.anthropic.com/research/statistical-approach-to-model-evals"
    abstract: "Methodology for rigorous statistical analysis of model evaluation results."
    source: ai_lab
    priority: 8
    subtopics: ["evaluation-methodology"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["methodology", "statistics"]

  - key: anthropic2025agents
    title: "Building Effective Agents"
    authors: ["Anthropic"]
    year: 2025
    url: "https://www.anthropic.com/research/building-effective-agents"
    abstract: "Best practices and design patterns for building AI agents."
    source: ai_lab
    priority: 7
    subtopics: ["agent-evaluation", "acquisition-guidance"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["practical-guidance"]

  - key: nber2025transformative
    title: "A Research Agenda for the Economics of Transformative AI"
    authors: ["NBER"]
    year: 2025
    url: "https://www.nber.org/papers/w34256"
    abstract: "NBER research agenda on economics of transformative AI, including productivity measurement frameworks."
    source: academic
    priority: 8
    subtopics: ["economic-impact", "evaluation-methodology"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["academic", "economics"]

  #############################################################################
  # STANDARDS AND FRAMEWORKS
  #############################################################################

  - key: nist2023airisk
    title: "AI Risk Management Framework"
    authors: ["NIST"]
    year: 2023
    url: "https://www.nist.gov/itl/ai-risk-management-framework"
    abstract: "NIST framework for AI risk management."
    source: government
    priority: 8
    subtopics: ["risk-management", "standards-frameworks"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["framework", "standards"]

  - key: aisi2025safeguards
    title: "Principles for Safeguard Evaluation"
    authors: ["UK AISI"]
    year: 2025
    url: "https://www.aisi.gov.uk/work/principles-for-safeguard-evaluation"
    abstract: "AISI principles for evaluating AI safeguards."
    source: government
    priority: 7
    subtopics: ["evaluation-methodology", "risk-management"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["government-eval"]

  - key: cset2025military
    title: "AI for Military Decision-Making"
    authors: ["CSET Georgetown"]
    year: 2025
    url: "https://cset.georgetown.edu/publication/ai-for-military-decision-making/"
    abstract: "Analysis of AI applications in military decision-making contexts."
    source: policy
    priority: 8
    subtopics: ["acquisition-guidance", "risk-management"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["military", "policy"]

  #############################################################################
  # TO PROCESS: HIGH-PRIORITY ARXIV PAPERS (from link dumps, Dec 2025)
  #############################################################################

  - key: arxiv2512_01786
    title: "arXiv:2512.01786"
    authors: ["Unknown"]
    year: 2025
    month: 12
    url: "https://arxiv.org/abs/2512.01786"
    abstract: "To be fetched and categorized"
    source: academic
    priority: 7
    subtopics: ["general"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["to-process", "recent"]

  - key: arxiv2510_26787
    title: "Remote Labor Index: Measuring AI Automation of Remote Work"
    authors: ["Scale AI", "CAIS"]
    year: 2025
    month: 10
    url: "https://arxiv.org/abs/2510.26787"
    abstract: "ArXiv version of Remote Labor Index paper"
    source: academic
    priority: 10
    subtopics: ["productivity-studies", "agent-evaluation"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["key-reference", "duplicate-of-scale2025remotelabor"]

  - key: arxiv2507_09089
    title: "Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity"
    authors: ["METR"]
    year: 2025
    month: 7
    url: "https://arxiv.org/abs/2507.09089"
    abstract: "ArXiv version of METR developer productivity study"
    source: academic
    priority: 10
    subtopics: ["productivity-studies", "human-ai-teaming"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["key-reference", "duplicate-of-metr2025developer"]

  - key: arxiv2510_04374
    title: "GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"
    authors: ["OpenAI"]
    year: 2025
    month: 10
    url: "https://arxiv.org/abs/2510.04374"
    abstract: "ArXiv version of OpenAI GDPval paper"
    source: academic
    priority: 10
    subtopics: ["productivity-studies", "evaluation-methodology"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["key-reference", "duplicate-of-openai2025gdpval"]

  #############################################################################
  # TIER 2: ADDITIONAL SOURCES FROM LINK PROCESSING (Dec 10, 2025)
  #############################################################################

  # Harvard/Wharton/BCG Studies
  - key: dellacqua2023jagged
    title: "Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality"
    authors: ["Dell'Acqua, Fabrizio", "McFowland, Edward", "Mollick, Ethan", "Lifshitz-Assaf, Hila", "Kellogg, Katherine", "Rajendran, Saran", "Krayer, Lisa", "Candelon, François", "Lakhani, Karim"]
    year: 2023
    url: "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4573321"
    abstract: |
      Field experiment with 244 BCG consultants. AI users finished 12.2% more tasks, 25.1% faster,
      40% higher quality. Introduced "Centaur" (clear human-AI division) and "Cyborg" (blended)
      work patterns. First major study of enterprise AI use. KEY CAVEAT: Tasks inside the
      "jagged frontier" showed gains; tasks outside showed AI failures and homogeneous outputs.
    source: academic
    priority: 10
    subtopics: ["productivity-studies", "human-ai-teaming", "jagged-frontier"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["key-reference", "field-experiment", "centaurs-cyborgs"]
    key_findings:
      - "12.2% more tasks, 25.1% faster, 40% higher quality with AI"
      - "Centaur vs Cyborg work patterns identified"
      - "AI outputs homogeneous without human intervention"
      - "Tasks outside jagged frontier showed AI failures"

  # RAND Military/Defense Sources
  - key: rand2025acquisition
    title: "Acquiring Generative AI for U.S. Department of Defense Influence Activities"
    authors: ["RAND Corporation"]
    year: 2025
    month: 7
    url: "https://www.rand.org/pubs/research_briefs/RBA3157-1.html"
    abstract: |
      Recommends DoD rapidly acquire GenAI but with enterprise-wide oversight. No current
      enterprisewide plan addresses GenAI implications. Emphasizes need for strategic, flexible
      acquisition approach with sustainment process.
    source: policy
    priority: 8
    subtopics: ["acquisition-guidance", "standards-frameworks"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["military", "acquisition", "policy"]

  - key: rand2025rma
    title: "An AI Revolution in Military Affairs? How Artificial Intelligence Could Reshape Future Warfare"
    authors: ["RAND Corporation"]
    year: 2025
    url: "https://www.rand.org/pubs/working_papers/WRA4004-1.html"
    abstract: |
      Evaluates how AI could reshape four military competitions: quantity vs quality, hiding vs
      finding, centralized vs decentralized C2, cyber offense vs defense. Argues AI-enabled warfare
      will reward mass, deception, mission command, and network resilience.
    source: policy
    priority: 7
    subtopics: ["acquisition-guidance", "risk-management"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["military", "strategy"]

  - key: rand2024limits
    title: "Understanding the Limits of Artificial Intelligence for Warfighters: Volume 1, Summary"
    authors: ["RAND Corporation"]
    year: 2024
    month: 1
    url: "https://www.rand.org/pubs/research_reports/RRA1722-1.html"
    abstract: |
      First in five-volume series on AI limitations for military use. KEY FINDINGS: Distributional
      shift degrades model performance - data must be recent for adaptive threats. AI is "tactically
      brilliant but strategically naive." Aimed at policymakers and acquisition professionals.
    source: policy
    priority: 9
    subtopics: ["acquisition-guidance", "evaluation-methodology", "jagged-frontier"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["military", "key-reference", "ai-limitations"]
    key_findings:
      - "Distributional shift degrades model performance"
      - "AI tactically brilliant but strategically naive"
      - "Data must be recent for adaptive threats"

  # IT Productivity Paradox / Economic Studies
  - key: wharton2025productivity
    title: "The Projected Impact of Generative AI on Future Productivity Growth"
    authors: ["Penn Wharton Budget Model"]
    year: 2025
    month: 9
    url: "https://budgetmodel.wharton.upenn.edu/issues/2025/9/8/projected-impact-of-generative-ai-on-future-productivity-growth"
    abstract: "Economic analysis of generative AI's projected impact on productivity growth."
    source: academic
    priority: 7
    subtopics: ["economic-impact"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["economics", "projections"]

  - key: brynjolfsson2023jcurve
    title: "The Productivity J-Curve: How Intangibles Complement General Purpose Technologies"
    authors: ["Brynjolfsson, Erik", "Rock, Daniel", "Syverson, Chad"]
    year: 2023
    url: null
    abstract: |
      Explains why early stages of major technology shifts show sluggish/negative productivity.
      Unlocking value requires massive complementary investments: reorganizing workflows,
      retraining staff, redesigning systems. These intangibles initially mask technology's
      true potential. Directly relevant to current AI adoption.
    source: academic
    priority: 8
    subtopics: ["economic-impact", "evaluation-methodology"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["economics", "IT-paradox", "complementary-investments"]
    key_findings:
      - "Productivity J-curve explains initial negative returns"
      - "Massive complementary investments required"
      - "Workflow redesign, retraining, infrastructure needed"

  - key: acemoglu2024skeptic
    title: "The Simple Macroeconomics of AI"
    authors: ["Acemoglu, Daron"]
    year: 2024
    url: "https://www.nber.org/papers/w32487"
    abstract: |
      Nobel laureate argues AI productivity gains will be far smaller and take far longer than
      optimists think. AI's impact on TFP growth remains small today - 0.01 percentage points
      in 2025. Important skeptical counterweight to industry claims.
    source: academic
    priority: 8
    subtopics: ["economic-impact"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["economics", "skeptical-view", "nobel-laureate"]

  # Benchmark Saturation and Evaluation Methodology
  - key: epoch2025rosetta
    title: "A Rosetta Stone for AI Benchmarks"
    authors: ["Epoch AI", "Google DeepMind"]
    year: 2025
    url: null
    abstract: |
      Novel statistical framework to unify diverse AI benchmarks into singular metric.
      Addresses benchmark saturation problem. Offers refined perspective on model comparisons
      across different evaluation types.
    source: research_org
    priority: 7
    subtopics: ["benchmarks", "evaluation-methodology"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["methodology", "benchmark-unification"]

  - key: staufer2025deprecation
    title: "Deprecating Benchmarks: Criteria and Framework"
    authors: ["Staufer et al."]
    year: 2025
    url: "https://arxiv.org/abs/2507.06434"
    abstract: |
      Argues evaluations should include information about deprecation circumstances.
      When is a benchmark no longer useful? Provides framework for benchmark lifecycle
      management.
    source: academic
    priority: 6
    subtopics: ["benchmarks", "evaluation-methodology"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["methodology", "benchmark-lifecycle"]

  - key: jetbrains2025ecosystem
    title: "The State of Developer Ecosystem 2025: Coding in the Age of AI"
    authors: ["JetBrains Research"]
    year: 2025
    month: 10
    url: "https://blog.jetbrains.com/research/2025/10/state-of-developer-ecosystem-2025/"
    abstract: |
      Large-scale survey on developer AI tool usage. 85% regularly use AI tools, 62% rely on
      at least one AI assistant. 78% believe AI improves productivity. Stack Overflow survey:
      only 16.3% said AI made them more productive "to a great extent"; 41.4% said little/no effect.
    source: industry
    priority: 7
    subtopics: ["productivity-studies", "human-ai-teaming"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["survey", "developer-tools"]

  - key: faros2025paradox
    title: "The AI Productivity Paradox Research Report"
    authors: ["Faros AI"]
    year: 2025
    url: "https://www.faros.ai/blog/ai-software-engineering"
    abstract: |
      75% of engineers use AI tools yet most organizations see no measurable performance gains.
      AI adoption associated with 9% increase in bugs per developer and 154% increase in average
      PR size. Important data on organization-level vs individual-level effects.
    source: industry
    priority: 8
    subtopics: ["productivity-studies", "economic-impact"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["organizational-effects", "negative-findings"]
    key_findings:
      - "75% of engineers use AI tools"
      - "9% increase in bugs per developer"
      - "154% increase in average PR size"
      - "No measurable org-level performance gains"

  - key: hbr2025workslop
    title: "AI-Generated 'Workslop' Is Destroying Productivity"
    authors: ["Harvard Business Review"]
    year: 2025
    month: 9
    url: "https://hbr.org/2025/09/ai-generated-workslop-is-destroying-productivity"
    abstract: "Analysis of how low-quality AI-generated content creates organizational inefficiency."
    source: industry
    priority: 6
    subtopics: ["productivity-studies", "risk-management"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["negative-effects", "organizational"]

  - key: mit2025adoption
    title: "MIT Media Lab AI ROI Study"
    authors: ["MIT Media Lab"]
    year: 2025
    url: null
    abstract: |
      Found 95% of organizations see no measurable return on AI investment. Important data point
      on enterprise AI adoption challenges despite high spending ($37B in 2025).
    source: academic
    priority: 7
    subtopics: ["economic-impact", "productivity-studies"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["negative-findings", "enterprise"]
    key_findings:
      - "95% of organizations see no measurable ROI"
      - "42% of AI pilots abandoned by end of 2024 (up from 17%)"

  #############################################################################
  # TIER 2: BATCH 2 - Additional Sources (Dec 10, 2025)
  #############################################################################

  # Anthropic Internal Study
  - key: anthropic2025internal
    title: "How AI Is Transforming Work at Anthropic"
    authors: ["Anthropic"]
    year: 2025
    month: 8
    url: "https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic"
    abstract: |
      Internal study: surveyed 132 engineers/researchers, 53 in-depth interviews, usage data.
      Staff use Claude in ~60% of work, self-report 50% productivity gains. Can fully delegate
      ~20% of tasks. KEY CAVEATS: Concerns about skill atrophy, reduced collaboration/mentorship,
      "Claude is now the first stop for questions that used to go to colleagues."
    source: ai_lab
    priority: 9
    subtopics: ["productivity-studies", "human-ai-teaming", "risk-management"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["internal-study", "skill-atrophy", "collaboration-concerns"]
    key_findings:
      - "60% of work uses Claude"
      - "50% self-reported productivity gains"
      - "20% of tasks fully delegatable"
      - "Concerns about skill atrophy and reduced mentorship"

  # User Heterogeneity Studies
  - key: brynjolfsson2023customerservice
    title: "Generative AI at Work"
    authors: ["Brynjolfsson, Erik", "Li, Danielle", "Raymond, Lindsey"]
    year: 2023
    url: "https://www.nber.org/papers/w31161"
    journal: "Quarterly Journal of Economics (2024)"
    abstract: |
      Study of 5,000+ customer support agents. AI assistance increases productivity 15% on average
      but with SUBSTANTIAL HETEROGENEITY. Less experienced/lower-skilled workers: 34% productivity
      increase. Most experienced/highest-skilled: small speed gains, SMALL DECLINES IN QUALITY.
      KEY: Benefits primarily accrue to novices; experts may be harmed.
    source: academic
    priority: 10
    subtopics: ["productivity-studies", "human-ai-teaming"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["key-reference", "heterogeneity", "skill-level-effects"]
    key_findings:
      - "15% average productivity increase"
      - "34% increase for less experienced workers"
      - "Small quality DECLINE for most experienced workers"
      - "Novices move up skill distribution; experts may be harmed"

  - key: noy2023writing
    title: "The Heterogeneous Productivity Effects of Generative AI"
    authors: ["Noy, Shakked", "Zhang, Whitney"]
    year: 2024
    url: "https://arxiv.org/abs/2403.01964"
    abstract: |
      Analysis of heterogeneous AI productivity effects. Gains predominantly favor individuals
      with lower or medium levels of experience, skill, or productivity. Less experienced users
      in Italy saw increased output quantity/quality; experienced users saw DECREASED productivity
      on routine tasks.
    source: academic
    priority: 8
    subtopics: ["productivity-studies", "human-ai-teaming"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["heterogeneity", "skill-level-effects"]

  # SWE-Bench and Agent Evaluation
  - key: runloop2025swebench
    title: "SWE-Bench Deep Dive: Unmasking the Limitations of a Popular Benchmark"
    authors: ["Runloop"]
    year: 2025
    url: "https://runloop.ai/blog/swe-bench-deep-dive-unmasking-the-limitations-of-a-popular-benchmark"
    abstract: |
      Critical analysis of SWE-Bench limitations: 32.67% have solution leakage in issue reports,
      31.08% have weak test cases, only Python repos, Django over-represented (45%+), likely
      data contamination. Models may be recalling memorized solutions rather than problem-solving.
    source: industry
    priority: 8
    subtopics: ["benchmarks", "agent-evaluation"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["benchmark-critique", "data-contamination"]
    key_findings:
      - "32.67% solution leakage in issue reports"
      - "31.08% weak test cases"
      - "Django over-represented at 45%+"
      - "Likely data contamination from training"

  - key: scale2025swebenchpro
    title: "SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?"
    authors: ["Scale AI"]
    year: 2025
    month: 9
    url: "https://arxiv.org/abs/2509.16941"
    pdf: "https://static.scale.com/uploads/654197dc94d34f66c0f5184e/SWEAP_Eval_Scale%20(9).pdf"
    abstract: |
      New benchmark addressing SWE-Bench limitations. KEY FINDING: Models scoring 70%+ on
      SWE-Bench Verified score only 23% on SWE-Bench Pro. GPT-5 and Claude Opus 4.1 both ~23%.
      Massive performance drop shows benchmark inflation in original. Still limited: Java, C++,
      Rust underrepresented; doesn't capture design, code review, documentation.
    source: research_org
    priority: 9
    subtopics: ["benchmarks", "agent-evaluation"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["key-reference", "benchmark-critique"]
    key_findings:
      - "70%+ on SWE-Bench Verified → 23% on SWE-Bench Pro"
      - "Massive performance drop reveals benchmark inflation"
      - "Real-world software engineering activities not captured"

  # Trust Calibration
  - key: patterns2024trust
    title: "Calibrating workers' trust in intelligent automated systems"
    authors: ["Cell Patterns"]
    year: 2024
    month: 8
    url: "https://www.cell.com/patterns/fulltext/S2666-3899(24)00187-9"
    abstract: |
      Systematic review of trust calibration in human-automation teaming. Overtrust leads to
      inappropriate/unsafe use; undertrust leads to underutilization. No definitive method for
      achieving calibration exists. Approaches include uncertainty visualization, explanations,
      soliciting human input, outcome feedback. Gaps remain in dynamic contexts.
    source: academic
    priority: 8
    subtopics: ["human-ai-teaming", "evaluation-methodology"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["trust-calibration", "methodology"]

  # Skill Atrophy / Deskilling
  - key: medtech2025deskilling
    title: "Deskilling in the Age of Artificial Intelligence: Implications, Mechanisms, and Mitigation Strategies"
    authors: ["MedTechNews"]
    year: 2025
    url: "https://medtechnews.uk/research-reports/deskilling-in-the-age-of-artificial-intelligence-implications-mechanisms-and-mitigation-strategies/"
    abstract: |
      Analysis of AI-driven skill atrophy. Low-complexity roles show AI-driven skill atrophy
      through over-reliance. GPS analogy: navigation AI reduces spatial reasoning. Junior/early
      career most at risk - don't form debugging, verification, reasoning habits. Medical evidence:
      colonoscopy detection rate fell 6 points after AI exposure.
    source: web
    priority: 7
    subtopics: ["human-ai-teaming", "risk-management"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["skill-atrophy", "deskilling", "risk"]
    key_findings:
      - "Low-complexity roles most vulnerable to skill atrophy"
      - "Juniors at highest risk - don't form foundational habits"
      - "Medical study: 6-point detection rate drop post-AI exposure"

  # MIT/NANDA AI Pilot Failure Study
  - key: mit2025genaidivide
    title: "The GenAI Divide: State of AI in Business 2025"
    authors: ["MIT NANDA Initiative"]
    year: 2025
    month: 8
    url: "https://fortune.com/2025/08/18/mit-report-95-percent-generative-ai-pilots-at-companies-failing-cfo/"
    abstract: |
      MIT NANDA report on enterprise AI adoption. KEY FINDING: Only 5% of AI pilots achieve
      rapid revenue acceleration; 95% stall with no P&L impact. Core barrier is "learning gap"
      not infrastructure/talent. 60% evaluated systems, only 20% reached pilot, only 5% reached
      production. Buy beats build (67% vs 22% success). Back-office automation has highest ROI
      but gets least investment. Shadow AI shows individual adoption outpaces enterprise.
    source: academic
    priority: 9
    subtopics: ["economic-impact", "acquisition-guidance"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["key-reference", "enterprise-adoption", "pilot-failure"]
    key_findings:
      - "95% of AI pilots fail to deliver P&L impact"
      - "60% evaluate → 20% pilot → 5% production"
      - "Buy beats build: 67% vs 22% success rate"
      - "Learning gap is core barrier, not tech"

  - key: hbr2025aiframework
    title: "Most AI Initiatives Fail. This 5-Part Framework Can Help."
    authors: ["Harvard Business Review"]
    year: 2025
    month: 11
    url: "https://hbr.org/2025/11/most-ai-initiatives-fail-this-5-part-framework-can-help"
    abstract: |
      Framework for successful AI adoption. Pilots fail due to lack of organizational scaffolding.
      Success requires aligned incentives, redesigned decision processes, AI-ready culture.
      Integration beats innovation - tools that plug into existing systems succeed while
      standalone solutions languish. Empower domain experts, not central AI labs.
    source: industry
    priority: 7
    subtopics: ["acquisition-guidance", "economic-impact"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["framework", "adoption-guidance"]

  # Ecological Validity
  - key: chi2025ecological
    title: "Ecological Validity Missing in AI-Assisted Clinical Decision Support Research"
    authors: ["CHI Italy 2025"]
    year: 2025
    url: "https://dl.acm.org/doi/10.1145/3750069.3750072"
    abstract: |
      Research on ecological validity in AI evaluation. Current AI-assisted decision support
      evaluations often lack ecological validity. "Patterns of overreliance on AI may result
      from experimental setups, rather than reflecting real-world behavior." Gap between
      experimental design and how decisions unfold in practice.
    source: academic
    priority: 7
    subtopics: ["evaluation-methodology"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["ecological-validity", "methodology"]

  # Collective Creativity Impact
  - key: pnas2024creativity
    title: "Generative AI enhances individual creativity but reduces the collective diversity of novel content"
    authors: ["PNAS"]
    year: 2024
    url: "https://pmc.ncbi.nlm.nih.gov/articles/PMC11244532/"
    abstract: |
      Study finding AI enhances individual creativity but reduces collective diversity.
      AI outputs are homogeneous - when many people use AI, aggregate output becomes more similar.
      Important implication for organizational use: individual gains may mask collective costs.
    source: academic
    priority: 7
    subtopics: ["productivity-studies", "risk-management"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["creativity", "homogeneity", "collective-effects"]

  # Motivation Impact
  - key: nature2025motivation
    title: "Human-generative AI collaboration enhances task performance but undermines human's intrinsic motivation"
    authors: ["Scientific Reports"]
    year: 2025
    url: "https://www.nature.com/articles/s41598-025-98385-2"
    abstract: |
      Study finding AI collaboration enhances performance but undermines intrinsic motivation.
      Important for long-term adoption and skill development considerations.
    source: academic
    priority: 6
    subtopics: ["human-ai-teaming", "risk-management"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["motivation", "psychological-effects"]

  #############################################################################
  # TIER 2: BATCH 3 - Military/Defense AI Evaluation (Dec 10, 2025)
  #############################################################################

  - key: fas2025benchmarking
    title: "Codifying and Expanding Continuous AI Benchmarking"
    authors: ["Federation of American Scientists"]
    year: 2025
    url: "https://fas.org/publication/codifying-expanding-continuous-ai-benchmarking/"
    abstract: |
      Recommends DoD embed benchmarking at pre-award acquisition stage and through entire lifecycle.
      Emphasizes internal adversarial stress testing (red-teaming) for mission-aligned evaluations.
      Argues for detecting risks, performance drift, and adversarial vulnerabilities before deployment.
    source: policy
    priority: 8
    subtopics: ["acquisition-guidance", "evaluation-methodology", "standards-frameworks"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["military", "benchmarking", "lifecycle-evaluation"]

  - key: scaleai2024dod
    title: "Scale AI DoD Benchmarking Initiative"
    authors: ["Scale AI", "DoD CDAO"]
    year: 2024
    month: 2
    url: "https://defensescoop.com/2024/02/20/scale-ai-pentagon-testing-evaluating-large-language-models/"
    abstract: |
      Pentagon's CDAO tapped Scale AI to develop trustworthy T&E for LLMs supporting military
      planning/decision-making. Measures quantitative benchmarks plus qualitative user feedback.
      Helps DoD mature T&E policies for generative AI by identifying models ready for military
      applications with accurate results using DoD terminology and knowledge bases.
    source: government
    priority: 8
    subtopics: ["evaluation-methodology", "acquisition-guidance"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["military", "benchmarking", "DoD"]

  - key: ndaa2025ai
    title: "FY2025 NDAA AI and Quantum Sciences Provisions"
    authors: ["U.S. Congress"]
    year: 2024
    month: 12
    url: "https://www.nextgov.com/emerging-tech/2024/12/fy2025-ndaa-angles-enhance-dods-ai-and-quantum-sciences-capabilities/401545/"
    abstract: |
      FY2025 NDAA authorizes $143.8B for science and tech R&D. Requires DoD to evaluate creating
      Federated AI-Enabled Weapon Systems Center of Excellence for analyzing AI weaponry, countermeasures,
      and training. Authorizes Defense Innovation Unit pilot program through 2028 for alternative
      T&E pathway to accelerate testing of technologies for warfighting capabilities.
    source: government
    priority: 7
    subtopics: ["standards-frameworks", "acquisition-guidance"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["military", "policy", "legislation"]

  - key: usaf2025doctrine
    title: "Air Force Doctrine Note 25-1: Artificial Intelligence"
    authors: ["U.S. Air Force"]
    year: 2025
    month: 4
    url: "https://www.doctrine.af.mil/Portals/61/documents/AFDN_25-1/AFDN%2025-1%20Artificial%20Intelligence.pdf"
    abstract: |
      Official Air Force doctrine on AI. July 2025 AI Action Plan identifies AI as enabling technology
      to transform both warfighting and back-office operations. States "United States must aggressively
      adopt AI within its Armed Forces." Important for understanding military AI adoption context.
    source: government
    priority: 7
    subtopics: ["acquisition-guidance", "standards-frameworks"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["military", "doctrine", "USAF"]

  - key: unidir2025military
    title: "AI in the Military Domain: A Briefing Note for States"
    authors: ["UNIDIR"]
    year: 2025
    month: 3
    url: "https://unidir.org/wp-content/uploads/2025/03/UNIDIR_AI_in_the_Military_Domain_A_briefing_note_for_States.pdf"
    abstract: "UN Institute for Disarmament Research briefing on military AI for policymakers."
    source: policy
    priority: 6
    subtopics: ["acquisition-guidance", "risk-management"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["military", "international", "policy"]

  - key: cnas2024practices
    title: "Military Artificial Intelligence Test and Evaluation Model Practices"
    authors: ["CNAS", "International Expert Group"]
    year: 2024
    month: 12
    url: "https://www.cnas.org/publications/commentary/military-artificial-intelligence-test-and-evaluation-model-practices"
    abstract: |
      White paper from American, Indian, European, and Chinese experts on T&E principles for
      military AI systems. Consensus on principles for safer, more secure, responsible AI weapons
      and military systems. Important for international standards perspective.
    source: policy
    priority: 7
    subtopics: ["evaluation-methodology", "standards-frameworks"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["military", "international", "standards"]

  #############################################################################
  # TIER 2: BATCH 3 - Productivity Studies & Economic Research (Dec 10, 2025)
  #############################################################################

  - key: stlouisfed2025productivity
    title: "The Impact of Generative AI on Work Productivity"
    authors: ["Federal Reserve Bank of St. Louis"]
    year: 2025
    month: 2
    url: "https://www.stlouisfed.org/on-the-economy/2025/feb/impact-generative-ai-work-productivity"
    abstract: |
      Federal Reserve survey finding 5.4% average time savings from AI (2.2 hours/week for 40-hour
      work week). 28% of all workers used generative AI at work. Usage rates stable Aug-Nov 2024.
      Important for baseline productivity estimates from representative sample.
    source: government
    priority: 8
    subtopics: ["productivity-studies", "economic-impact"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["survey", "representative-sample", "federal-reserve"]

  - key: pwc2025barometer
    title: "PwC Global AI Jobs Barometer 2025"
    authors: ["PwC"]
    year: 2025
    url: "https://www.pwc.com/gx/en/news-room/press-releases/2025/ai-linked-to-a-fourfold-increase-in-productivity-growth.html"
    abstract: |
      AI-skilled workers see 56% wage premium in 2024 (double the 25% in previous year). Job
      availability grew 38% in AI-exposed roles. Productivity growth nearly quadrupled in
      AI-exposed industries since GenAI proliferation in 2022.
    source: industry
    priority: 7
    subtopics: ["economic-impact", "productivity-studies"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["wage-premium", "labor-market", "industry-effects"]

  - key: oecd2025productivity
    title: "Unlocking Productivity with Generative AI: Evidence from Experimental Studies"
    authors: ["OECD"]
    year: 2025
    month: 7
    url: "https://www.oecd.org/en/blogs/2025/07/unlocking-productivity-with-generative-ai-evidence-from-experimental-studies.html"
    abstract: |
      OECD analysis of experimental evidence. KEY FINDING: Less-experienced/lower-skilled individuals
      see largest productivity gains. AI facilitates on-the-job learning, provides support/feedback,
      helps lower-skilled workers bridge gap with skilled peers. Important for understanding
      heterogeneity in AI benefits.
    source: policy
    priority: 8
    subtopics: ["productivity-studies", "human-ai-teaming"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["heterogeneity", "skill-effects", "OECD"]

  - key: mckinsey2025workplace
    title: "AI in the Workplace: A Report for 2025"
    authors: ["McKinsey"]
    year: 2025
    url: "https://www.mckinsey.com/capabilities/tech-and-ai/our-insights/superagency-in-the-workplace-empowering-people-to-unlock-ais-full-potential-at-work"
    abstract: |
      Survey of 3,613 employees and 238 C-level executives (Oct-Nov 2024). 92% of companies plan
      to increase AI investments over next 3 years but only 1% of leaders call companies "mature"
      on deployment spectrum. Important data on adoption maturity gap.
    source: industry
    priority: 7
    subtopics: ["economic-impact", "acquisition-guidance"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["survey", "enterprise-adoption", "maturity-gap"]

  - key: dallasfed2025productivity
    title: "Advances in AI Will Boost Productivity, Living Standards Over Time"
    authors: ["Federal Reserve Bank of Dallas"]
    year: 2025
    month: 6
    url: "https://www.dallasfed.org/research/economics/2025/0624"
    abstract: "Federal Reserve analysis of long-term AI productivity impacts."
    source: government
    priority: 6
    subtopics: ["economic-impact"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["federal-reserve", "long-term"]

  #############################################################################
  # TIER 2: BATCH 3 - Training Intervention Studies (Dec 10, 2025)
  #############################################################################

  - key: noy2023experiment
    title: "Experimental Evidence on the Productivity Effects of Generative AI"
    authors: ["Noy, Shakked", "Zhang, Whitney"]
    year: 2023
    journal: "Science"
    url: "https://www.science.org/doi/10.1126/science.adh2586"
    abstract: |
      Landmark study published in Science. 453 college-educated professionals given writing tasks.
      Half exposed to ChatGPT. Results: 40% decrease in time taken, 18% increase in output quality.
      Inequality between workers decreased. Important RCT evidence with high-quality publication.
    source: academic
    priority: 9
    subtopics: ["productivity-studies", "human-ai-teaming"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["key-reference", "RCT", "Science-journal"]
    key_findings:
      - "40% decrease in task completion time"
      - "18% increase in output quality"
      - "Reduced inequality between workers"

  - key: stanfordmit2023support
    title: "Generative AI at Work (Customer Support Study)"
    authors: ["Brynjolfsson, Erik", "Li, Danielle", "Raymond, Lindsey"]
    year: 2023
    url: "https://www.gsb.stanford.edu/insights/generative-ai-can-boost-productivity-without-replacing-workers"
    related_url: "https://www.cnbc.com/2023/04/25/stanford-and-mit-study-ai-boosted-worker-productivity-by-14percent.html"
    abstract: |
      Study of 5,000+ customer support agents at Fortune 500 firm over one year. AI tools boosted
      productivity 14% on average. Novice/low-skilled workers: 35% faster, performed like agents
      with 6 months more experience. KEY: "Those who use it will replace those who don't."
    source: academic
    priority: 9
    subtopics: ["productivity-studies", "human-ai-teaming"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["key-reference", "field-study", "heterogeneity"]
    key_findings:
      - "14% average productivity boost"
      - "35% improvement for novice workers"
      - "2 months experience performed like 6 months unsupported"

  - key: mitsloan2024boundary
    title: "How Generative AI Can Boost Highly Skilled Workers' Productivity"
    authors: ["MIT Sloan"]
    year: 2024
    url: "https://mitsloan.mit.edu/ideas-made-to-matter/how-generative-ai-can-boost-highly-skilled-workers-productivity"
    abstract: |
      AI can improve highly skilled worker performance by nearly 40% when used within capability
      boundary. KEY CAVEAT: When AI used OUTSIDE boundary, performance DROPS 19 percentage points.
      Validates jagged frontier concept. Experts need to validate AI and exert cognitive effort.
    source: academic
    priority: 8
    subtopics: ["productivity-studies", "jagged-frontier"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["boundary-effects", "expert-performance"]
    key_findings:
      - "40% improvement within AI capability boundary"
      - "19pp performance DROP outside boundary"
      - "Experts must continue to validate AI outputs"

  - key: inc2025training
    title: "How AI Training Can Lead to Productivity Gains"
    authors: ["Inc. Magazine"]
    year: 2025
    url: "https://www.inc.com/bruce-crumley/how-ai-training-can-lead-to-productivity-gains/91260858"
    abstract: |
      Survey data: 68% of employees received NO AI training in past 12 months. Impact: trained
      employees save 11 hours/week vs 5 hours for untrained (2x difference). 93% of trained
      workers regularly use AI vs 57% untrained. "Closing AI training gap is fastest way to
      unlock measurable return."
    source: industry
    priority: 7
    subtopics: ["acquisition-guidance", "productivity-studies"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["training", "adoption", "intervention"]
    key_findings:
      - "68% of employees received no AI training"
      - "Trained workers save 2x more time (11 vs 5 hours/week)"
      - "93% adoption with training vs 57% without"

  #############################################################################
  # TIER 2: BATCH 3 - Agentic AI Evaluation (Dec 10, 2025)
  #############################################################################

  - key: agentbench2024
    title: "AgentBench: Evaluating LLMs as Agents"
    authors: ["THUDM"]
    year: 2024
    journal: "ICLR 2024"
    url: "https://github.com/THUDM/AgentBench"
    abstract: |
      First benchmark designed to evaluate LLM-as-Agent across 8 distinct environments. Tests
      autonomous agent capabilities in web navigation, code execution, database queries, etc.
      Important baseline for agentic evaluation.
    source: academic
    priority: 7
    subtopics: ["agent-evaluation", "benchmarks"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["agent-benchmark", "ICLR"]

  - key: sierra2025taubench
    title: "τ-Bench: Benchmarking AI Agents for the Real World"
    authors: ["Sierra AI"]
    year: 2025
    url: "https://sierra.ai/blog/benchmarking-ai-agents"
    abstract: |
      Benchmark evaluating AI agent performance/reliability in real-world settings with dynamic
      user and tool interaction. Tests complex tasks with LLM-simulated users and tools.
      Addresses gap in measuring agent reliability in dynamic human-in-the-loop scenarios.
    source: ai_lab
    priority: 8
    subtopics: ["agent-evaluation", "benchmarks"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["agent-benchmark", "human-in-loop"]

  - key: ibm2025agents
    title: "The Future of AI Agent Evaluation"
    authors: ["IBM Research"]
    year: 2025
    url: "https://research.ibm.com/blog/AI-agent-benchmarks"
    abstract: |
      IBM Research analysis of AI agent evaluation challenges. Static benchmarks for LLMs don't
      apply cleanly to agents. Agent evaluation must consider dynamic outputs/contexts, uncertainty,
      multi-step task completion. Proposes evaluation framework considering adaptability, recovery,
      human feedback alignment.
    source: ai_lab
    priority: 7
    subtopics: ["agent-evaluation", "evaluation-methodology"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["methodology", "agent-evaluation"]

  - key: deloitte2025agents
    title: "Autonomous Generative AI Agents Still Under Development"
    authors: ["Deloitte"]
    year: 2025
    url: "https://www.deloitte.com/us/en/insights/industry/technology/technology-media-and-telecom-predictions/2025/autonomous-generative-ai-agents-still-under-development.html"
    abstract: |
      Deloitte technology predictions. Task-specific AI agents present in <5% of applications in
      2025, expected in 40% by 2026. AutoGen adopted by 40%+ of Fortune 100. Devin resolves
      ~14% of GitHub issues (2x better than chatbots, but not fully autonomous).
    source: industry
    priority: 6
    subtopics: ["agent-evaluation", "economic-impact"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["predictions", "adoption"]

  #############################################################################
  # TIER 2: BATCH 3 - Trust & Overreliance Research (Dec 10, 2025)
  #############################################################################

  - key: chb2024overreliance
    title: "Trust and Reliance on AI: An Experimental Study on the Extent and Costs of Overreliance"
    authors: ["Computers in Human Behavior"]
    year: 2024
    url: "https://www.sciencedirect.com/science/article/pii/S0747563224002206"
    abstract: |
      Incentivized behavioral experiment finding mere knowledge of advice being AI-generated
      causes overreliance, even when contradicting contextual information and own assessment.
      Overreliance leads to inefficient outcomes and undesired third-party effects. Emphasizes
      AI literacy and trust calibration for productive deployment.
    source: academic
    priority: 8
    subtopics: ["human-ai-teaming", "risk-management"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["overreliance", "trust", "behavioral-experiment"]
    key_findings:
      - "Mere knowledge of AI source causes overreliance"
      - "Users follow AI even contradicting own assessment"
      - "AI literacy critical for productive deployment"

  - key: cset2024bias
    title: "AI Safety and Automation Bias"
    authors: ["CSET Georgetown"]
    year: 2024
    month: 11
    url: "https://cset.georgetown.edu/publication/ai-safety-and-automation-bias/"
    pdf: "https://cset.georgetown.edu/wp-content/uploads/CSET-AI-Safety-and-Automation-Bias.pdf"
    abstract: |
      Policy brief on automation bias. KEY: "Human-in-the-loop" cannot prevent all accidents/errors.
      Properly calibrating technical and human fail-safes poses best chance for mitigating AI risks.
      Case studies show risks across domains. Important for military T&E considerations.
    source: policy
    priority: 8
    subtopics: ["risk-management", "human-ai-teaming"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["automation-bias", "safety", "policy"]

  - key: acm2024trust
    title: "A Systematic Review on Fostering Appropriate Trust in Human-AI Interaction"
    authors: ["ACM Journal on Responsible Computing"]
    year: 2024
    url: "https://dl.acm.org/doi/10.1145/3696449"
    abstract: |
      Systematic review of trust calibration research. Rather than higher trust, CALIBRATED trust
      is critical. Higher transparency may negatively affect trust and induce complacency/overreliance.
      Mixed findings on effectiveness of explanations. Gaps remain in dynamic contexts.
    source: academic
    priority: 7
    subtopics: ["human-ai-teaming", "evaluation-methodology"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["trust-calibration", "systematic-review"]

  - key: springer2025bias
    title: "Exploring Automation Bias in Human-AI Collaboration: A Review and Implications for Explainable AI"
    authors: ["AI & Society"]
    year: 2025
    url: "https://link.springer.com/article/10.1007/s00146-025-02422-7"
    abstract: |
      As AI embeds in high-stakes domains (healthcare, law, public administration), automation
      bias emerges as critical challenge. Proposes shift: rather than humans supervising AI,
      let humans make primary decisions with AI as second-opinion system alerting to potential errors.
    source: academic
    priority: 7
    subtopics: ["human-ai-teaming", "evaluation-methodology"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["automation-bias", "design-recommendation"]

  - key: arxiv2025falseconfidence
    title: "Students' Reliance on AI in Higher Education: The False-Confidence Loop"
    authors: ["ArXiv"]
    year: 2025
    url: "https://arxiv.org/html/2506.13845v1"
    abstract: |
      Evidence for "false-confidence loop" in human-AI interaction. Overreliant users trusted
      system and felt satisfied while unknowingly accepting errors that undermined performance.
      Programming self-efficacy, literacy, and need for cognition associated with calibrated trust.
    source: academic
    priority: 6
    subtopics: ["human-ai-teaming", "risk-management"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["false-confidence", "overreliance"]

  #############################################################################
  # TIER 2: BATCH 3 - NIST Standards & Frameworks (Dec 10, 2025)
  #############################################################################

  - key: nist2024aria
    title: "NIST ARIA Program: Assessing Risks and Impacts of AI"
    authors: ["NIST"]
    year: 2024
    month: 5
    url: "https://www.nist.gov/news-events/news/2024/05/nist-launches-aria-new-program-advance-sociotechnical-testing-and"
    related_url: "https://ai-challenges.nist.gov/aria"
    abstract: |
      ARIA assesses societal risks/impacts of AI systems in realistic settings. Three evaluation
      levels: model testing, red-teaming, field testing. Moves beyond accuracy to technical and
      contextual robustness. Expands on AI RMF. Timeline: pilot Dec 2024-Jan 2025, analysis Feb-May
      2025, report Summer/Fall 2025.
    source: government
    priority: 8
    subtopics: ["standards-frameworks", "evaluation-methodology"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["NIST", "standards", "sociotechnical"]

  - key: nist2025zerodrafts
    title: "NIST AI Standards Zero Drafts Pilot Project"
    authors: ["NIST"]
    year: 2025
    month: 3
    url: "https://www.nist.gov/artificial-intelligence/ai-research/nists-ai-standards-zero-drafts-pilot-project-accelerate"
    abstract: |
      Pilot project to accelerate AI standards creation. Initial topics: AI T&E verification/validation
      and documentation of AI models/datasets. Goal: flexible high-level framework for companies
      to design own AI testing procedures. Not prescribing exact methods; offering structure around
      key terms, lifecycle stages, guiding principles aligned with international standards.
    source: government
    priority: 7
    subtopics: ["standards-frameworks", "evaluation-methodology"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["NIST", "standards", "verification-validation"]

  - key: nist2024genai
    title: "2024 NIST GenAI Pilot Study: Text-to-Text Evaluation"
    authors: ["NIST"]
    year: 2024
    url: "https://www.nist.gov/publications/2024-nist-genai-pilot-study-text-text-evaluation-overview-and-results"
    abstract: |
      NIST pilot study evaluating text-to-text generation and discrimination tasks. Assessed using
      AUC and Brier scores. AI-generated summaries increasingly resemble human writing but detection
      models remain reasonably effective. Important baseline for GenAI evaluation methodology.
    source: government
    priority: 6
    subtopics: ["evaluation-methodology", "benchmarks"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["NIST", "evaluation", "text-generation"]

  #############################################################################
  # TIER 2: BATCH 3 - Post-Deployment Monitoring (Dec 10, 2025)
  #############################################################################

  - key: adalovelace2025monitoring
    title: "Safe Beyond Sale: Post-Deployment Monitoring of AI"
    authors: ["Ada Lovelace Institute"]
    year: 2025
    url: "https://www.adalovelaceinstitute.org/blog/post-deployment-monitoring-of-ai/"
    abstract: |
      Analysis of post-deployment AI monitoring. Still nascent field - both governments and AI
      companies have responsibilities to develop monitoring ecosystem. Notes "irony of information
      age": companies collecting most data are least monitored by regulators. Important for
      lifecycle evaluation considerations.
    source: policy
    priority: 7
    subtopics: ["evaluation-methodology", "risk-management"]
    read: false
    notes_file: null
    added: "2025-12-10"
    tags: ["monitoring", "post-deployment", "policy"]

  #############################################################################
  # TIER 1: RECENT CRITICAL ADDITIONS (Dec 31, 2025)
  # High-priority recent papers (Oct-Dec 2025) from link-dumps database review
  #############################################################################

  - key: bean2025construct
    title: "Measuring what Matters: Construct Validity in Large Language Model Benchmarks"
    authors: ["Bean, Andrew M.", "Kearns, Ryan Othniel", "Romanou, Angelika", "Hafner, Franziska Sofia", "Mayne, Harry", "Batzner, Jan", "Foroutan, "Negar", "Schmitz, Chris", "Korgul, Karolina", "Batra, Hunar", "Deb, Oishi", "Beharry, Emma", "Emde, Cornelius", "Foster, Thomas", "Gausen, Anna", "Grandury, María", "Han, Simeng", "Hofmann, Valentin", "Ibrahim, Lujain", "Kim, Hazel", "Kirk, Hannah Rose", "Lin, Fangru"]
    year: 2025
    month: 11
    url: "https://arxiv.org/abs/2511.04703"
    arxiv: "https://arxiv.org/abs/2511.04703"
    abstract: |
      Landmark systematic review of 445 LLM benchmarks by 29 expert reviewers from leading NLP and ML
      conferences. Identifies pervasive construct validity issues - benchmarks often don't measure what
      they claim to measure. Finds systematic patterns undermining validity: disconnect between claimed
      phenomena (safety, robustness) and actual tasks, scoring metrics that don't capture constructs,
      missing validation of whether benchmarks measure intended attributes. Provides 8 key recommendations
      and detailed actionable guidance for developing valid benchmarks. Essential for understanding
      benchmark-utility gap and evaluation methodology.
    source: academic
    priority: 10
    subtopics: ["benchmarks", "evaluation-methodology"]
    read: true
    notes_file: "../Claude-Notes/Paper-Summaries/bean2025construct.md"
    added: "2025-12-31"
    read_date: "2025-12-31"
    tags: ["key-reference", "systematic-review", "construct-validity", "benchmark-critique"]
    key_findings:
      - "Systematic patterns undermining validity across 445 benchmarks"
      - "Disconnect between claimed phenomena and actual tasks measured"
      - "8 actionable recommendations for valid benchmark design"
      - "29 expert reviewers, comprehensive methodology"

  - key: pan2025agents
    title: "Measuring Agents in Production"
    authors: ["Pan, Melissa Z.", "Arabzadeh, Negar", "Cogo, Riccardo", "Zhu, Yuxuan", "Xiong, Alexander", "Agrawal, Lakshya A.", "Mao, Huanzhi", "Shen, Emma", "Pallerla, Sid", "Patel, Liana", "Liu, Shu", "Shi, Tianneng", "Liu, Xiaoyuan", "Davis, Jared Quincy", "Lacavalla, Emmanuele", "Basile, Alessandro", "Yang, Shuyi", "Castro, Paul", "Kang, Daniel", "Gonzalez, Joseph E.", "Sen, Koushik", "Song, Dawn", "Stoica, Ion", "Zaharia, Matei", "Ellis, Marquita"]
    year: 2025
    month: 12
    url: "https://arxiv.org/abs/2512.04123"
    arxiv: "https://arxiv.org/abs/2512.04123"
    abstract: |
      First large-scale systematic study of AI agents in production environments. Surveyed 306
      practitioners and conducted 20 in-depth case studies across 26 domains. KEY FINDINGS: Production
      agents use simple, controllable approaches - 68% execute ≤10 steps before requiring human
      intervention, 70% rely on prompting off-the-shelf models vs fine-tuning, 74% depend primarily
      on human evaluation. Reveals massive gap between research prototypes (long-horizon autonomous
      agents) and production reality (short chains with heavy human oversight). Agent evaluation
      remains unsolved in practice. Essential for Part 4 (Practical Guidance) and understanding
      deployment constraints.
    source: academic
    priority: 10
    subtopics: ["agent-evaluation", "productivity-studies", "acquisition-guidance"]
    read: true
    notes_file: "../Claude-Notes/Paper-Summaries/pan2025agents.md"
    added: "2025-12-31"
    read_date: "2025-12-31"
    tags: ["key-reference", "empirical-anchor", "production-reality", "agent-deployment"]
    key_findings:
      - "68% of production agents execute ≤10 steps before human intervention"
      - "70% use prompting vs fine-tuning (controllability > performance)"
      - "74% rely on human evaluation (automated metrics insufficient)"
      - "Massive gap between research prototypes and production reality"

  - key: brand2025benchmarking
    title: "Why Benchmarking is Hard"
    authors: ["Brand, Florian", "Denain, Jean-Stanislas"]
    year: 2025
    month: 12
    url: "https://epoch.ai/gradient-updates/why-benchmarking-is-hard"
    abstract: |
      Epoch AI analysis of practical benchmarking challenges. Documents how numerous variables affect
      final scores across benchmark setup (prompts, sampling parameters, scaffolds, execution
      environments, scoring methods) and API access (rate limits, truncation, timeouts, provider
      differences). KEY FINDING: Scaffolds alone cause 11-15% variance on SWE-bench Verified. Selecting
      different API providers yields noticeably divergent results for identical models. Variables
      compound across evaluation stack causing final scores to diverge significantly from developer-reported
      numbers. Creates resource barriers particularly burdening smaller organizations - academics and
      hobbyists lack resources to navigate multiple API implementations, slowing independent assessment.
      Critical for understanding why benchmark results don't replicate.
    source: research_org
    priority: 9
    subtopics: ["benchmarks", "evaluation-methodology"]
    read: true
    read_date: "2025-12-31"
    notes_file: "../Claude-Notes/Paper-Summaries/brand2025benchmarking.md"
    added: "2025-12-31"
    tags: ["benchmark-critique", "reproducibility", "methodology"]
    key_findings:
      - "Scaffolds cause 11-15% variance on SWE-bench Verified"
      - "API provider selection yields divergent results for identical models"
      - "Variables compound across evaluation stack"
      - "Resource barriers impact independent evaluation capacity"

  - key: ho2025rosetta
    title: "A Rosetta Stone for AI Benchmarks"
    authors: ["Ho, Anson", "Denain, Jean-Stanislas", "Atanasov, David", "Albanie, Samuel", "Shah, Rohin"]
    year: 2025
    month: 11
    url: "https://arxiv.org/abs/2512.00193"
    arxiv: "https://arxiv.org/abs/2512.00193"
    abstract: |
      Novel statistical framework addressing benchmark saturation problem. Most AI benchmarks saturate
      within years or months, making it hard to study long-run capability trends. Framework "stitches
      benchmarks together" putting model capabilities and benchmark difficulties on single numerical
      scale - acts as "Rosetta Stone" allowing comparison of models across wide range of abilities
      and time even if not evaluated on same benchmarks. Works without assuming how capabilities
      evolve or scale with compute. Three applications: (1) measure speed of AI progress over time,
      (2) forecast future capabilities, (3) compare models across benchmark families. Important for
      Part 1 discussion of benchmark lifecycle and saturation.
    source: academic
    priority: 8
    subtopics: ["benchmarks", "evaluation-methodology"]
    read: true
    read_date: "2025-12-31"
    notes_file: "../Claude-Notes/Paper-Summaries/ho2025rosetta.md"
    added: "2025-12-31"
    tags: ["methodology", "benchmark-unification", "forecasting"]
    key_findings:
      - "Statistical framework unifying diverse benchmarks on single scale"
      - "Enables cross-benchmark model comparison despite saturation"
      - "Supports capability forecasting and long-term trend analysis"
      - "Rohin Shah (DeepMind AI safety) involvement"

  - key: kim2025scaling
    title: "Towards a Science of Scaling Agent Systems"
    authors: ["Kim, Yubin", "Gu, Ken", "Park, Chanwoo", "Park, Chunjong", "Schmidgall, Samuel", "Heydari, A. Ali", "Yan, Yao", "Zhang, Zhihan", "Zhuang, Yuchen", "Malhotra, Mark", "Liang, Paul Pu", "Park, Hae Won", "Yang, Yuzhe", "Xu, Xuhai", "Du, Yilun", "Patel, Shwetak", "Althoff, Tim", "McDuff, Daniel", "Liu, Xin"]
    year: 2025
    month: 12
    url: "https://arxiv.org/abs/2512.08296"
    arxiv: "https://arxiv.org/abs/2512.08296"
    abstract: |
      Proposes scientific framework for understanding how agent systems scale. 19 authors from MIT,
      Google, Meta, UW, and other institutions. Addresses gap in understanding multi-agent system
      scaling compared to single-model scaling laws. Relevant for Part 6 (Research Agenda) and future
      acquisition considerations as DoD explores multi-agent collaborative systems. Connects agent
      architecture choices to scaling behavior and emergent capabilities.
    source: academic
    priority: 9
    subtopics: ["agent-evaluation", "scaling-laws"]
    read: true
    read_date: "2025-12-31"
    notes_file: "../Claude-Notes/Paper-Summaries/kim2025scaling.md"
    added: "2025-12-31"
    tags: ["agent-systems", "multi-agent", "scaling", "methodology"]
    key_findings:
      - "Tool-coordination tradeoff under fixed budgets"
      - "Diminishing returns from coordination above 45% single-agent performance"
      - "Centralized coordination: 80.8% improvement on parallelizable tasks"
      - "Multi-agent degrades performance on sequential reasoning tasks"

  - key: lmarena2025expert
    title: "Arena Expert Model Comparison"
    authors: ["LMArena Team"]
    year: 2025
    month: 12
    url: "https://news.lmarena.ai/arena-expert-model-comparison/"
    abstract: |
      Analysis showing expert users rate AI models differently than general users. Critical for
      understanding how user skill and expertise affects perceived AI utility. Expert vs novice
      rating divergence has implications for who should evaluate systems and how to interpret
      evaluation results. Directly relevant to heterogeneity findings in Brynjolfsson and Noy
      studies showing AI benefits vary by skill level. Important for acquisition guidance on
      selecting appropriate evaluators and interpreting benchmarks.
    source: research_org
    priority: 8
    subtopics: ["benchmarks", "evaluation-methodology", "human-ai-teaming"]
    read: true
    read_date: "2025-12-31"
    notes_file: "../Claude-Notes/Paper-Summaries/lmarena2025expert.md"
    added: "2025-12-31"
    tags: ["user-heterogeneity", "expert-evaluation", "skill-effects"]
    key_findings:
      - "24-point gap between expert and novice preferences (thinking +15, non-thinking -9)"
      - "Claude Opus 4.5: +85 expert advantage (highest of any model)"
      - "Anthropic models: +22 average expert advantage (company-level quality differences)"
      - "Expert preferences diverge markedly from general user benchmarks"

  - key: anthropic2025workdec
    title: "How AI is Transforming Work at Anthropic (December 2025 Update)"
    authors: ["Huang, Saffron", "Seethor, Bryan", "Durmus, Esin", "Handa, Kunal", "McCain, Miles", "Stern, Michael", "Ganguli, Deep"]
    year: 2025
    month: 12
    url: "https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic"
    abstract: |
      Updated internal study examining AI adoption at Anthropic (Feb-Aug 2025 data). 132 employee
      survey, 53 qualitative interviews, analysis of 200,000 Claude Code transcripts. KEY FINDINGS:
      Engineers use Claude in ~60% of work (up from 28%), report 50% productivity gains. Can fully
      delegate ~20% of tasks. Claude Code autonomy doubled (10→20 consecutive actions). Complex task
      usage increased (code design: 1%→10%, feature implementation: 14%→37%). CRITICAL CONCERNS:
      Skill atrophy in core competencies, reduced collaboration/mentorship (Claude replaces colleague
      questions), career uncertainty. 44% of Claude work is tasks employees wouldn't have enjoyed.
      Important update to earlier Aug 2025 version (anthropic2025internal) with substantially more
      data and detail.
    source: ai_lab
    priority: 9
    subtopics: ["productivity-studies", "human-ai-teaming", "risk-management"]
    read: true
    read_date: "2025-12-31"
    notes_file: "../Claude-Notes/Paper-Summaries/anthropic2025workdec.md"
    added: "2025-12-31"
    tags: ["internal-study", "skill-atrophy", "collaboration-concerns", "updated-data"]
    key_findings:
      - "60% of work uses Claude (doubled from 28% prior year)"
      - "50% self-reported productivity gains"
      - "27% of Claude work is entirely new tasks (capability expansion)"
      - "Autonomy doubled (10→20 consecutive actions)"
      - "Concerns: skill atrophy, reduced mentorship, career uncertainty"

  - key: benchek2025ace
    title: "The AI Consumer Index (ACE)"
    authors: ["Benchek, Julien", "Shetty, Rohit", "Hunsberger, Benjamin", "Arun, Ajay", "Richards, Zach", "Foody, Brendan", "Nitski, Osvald", "Vidgen, Bertie"]
    year: 2025
    month: 12
    url: "https://arxiv.org/abs/2512.04921"
    arxiv: "https://arxiv.org/abs/2512.04921"
    abstract: |
      First benchmark assessing frontier AI models on everyday consumer tasks. 400 test cases (hidden
      heldout set) + 80 open-sourced devset cases across four activities: shopping, food, gaming, DIY.
      Novel grading methodology dynamically checks whether responses are grounded in retrieved web
      sources. KEY FINDINGS: Top model (GPT-5 Thinking=High) scores only 56.1%. Domain variation
      significant - shopping domain under 50%. Low scores reveal gaps between frontier capabilities
      and practical consumer utility. Complements productivity-focused evaluations. Relevant for
      understanding breadth of AI economic impact and dual-use applications.
    source: ai_lab
    priority: 7
    subtopics: ["benchmarks", "evaluation-methodology"]
    read: true
    read_date: "2025-12-31"
    notes_file: "../Claude-Notes/Paper-Summaries/benchek2025ace.md"
    added: "2025-12-31"
    tags: ["consumer-tasks", "grounding", "practical-utility"]
    key_findings:
      - "Top model (GPT-5 Thinking=High): 56.1% (43.9% failure rate)"
      - "Shopping domain: <50% (significant domain variation)"
      - "Grounding-based evaluation methodology (check against sources)"
      - "Gap between frontier capabilities and practical consumer utility"

  #############################################################################
  # TIER 2: MEDIUM-PRIORITY RECENT ADDITIONS (Dec 31, 2025)
  # Safety & alignment papers relevant for Part 5 (T&E Considerations)
  #############################################################################

  - key: anthropic2025vend2
    title: "Project Vend: Phase Two"
    authors: ["Anthropic Research Team"]
    year: 2025
    month: 12
    url: "https://www.anthropic.com/research/project-vend-2"
    abstract: |
      Anthropic's second phase testing Claude as autonomous AI shopkeeper "Claudius" running vending
      machine business. Phase 2 improvements: Sonnet 4.0/4.5, better tools (CRM, inventory), procedural
      constraints. Achieved consistent profitability across multiple locations (vs Phase 1 losses).
      CRITICAL FINDINGS: Despite improvements, remained vulnerable to social engineering - attempted
      illegal futures contracts, unlawful employment, nearly accepted false voting to replace CEO.
      Root cause: models' helpfulness-over-profit bias creates exploitable vulnerabilities. KEY LESSON:
      Capability improvements don't automatically solve alignment issues. Effective guardrails require
      balancing general oversight, economic viability, and manipulation robustness. Real-world case
      study for Part 5 (T&E) and Part 4 (acquisition) on agent deployment challenges.
    source: ai_lab
    priority: 8
    subtopics: ["agent-evaluation", "risk-management", "ai-safety"]
    read: false
    notes_file: "../../link-dumps/notes/anthropic-project-vend-2.md"
    added: "2025-12-31"
    tags: ["autonomous-agents", "alignment-challenges", "real-world-deployment", "case-study"]
    key_findings:
      - "Achieved profitability but remained vulnerable to social engineering"
      - "Helpfulness bias creates security vulnerabilities"
      - "Capability ≠ alignment: improvements in one don't solve the other"
      - "Guardrails need holistic design (oversight + viability + robustness)"

  - key: tomasev2025distributional
    title: "Distributional AGI Safety"
    authors: ["Tomašev, Nenad", "Franklin, Matija", "Jacobs, Julian", "Krier, Sébastien", "Osindero, Simon"]
    year: 2025
    month: 12
    url: "https://arxiv.org/abs/2512.16856"
    arxiv: "https://arxiv.org/abs/2512.16856"
    abstract: |
      Challenges conventional single-system AGI safety focus. Proposes alternative scenario: general
      capability manifests through coordination of sub-AGI agents with complementary skills. Given
      rapid deployment of agents with tool-use and communication, advocates distributional safety
      framework. Proposes "virtual agentic sandbox economies" with market mechanisms, auditability,
      reputation management, oversight to mitigate collective risks. KEY SHIFT: Safety research must
      address emergent risks from distributed, coordinated AI systems, not just individual alignment.
      Relevant for Part 5 (T&E) understanding multi-agent system risks and Part 6 (research agenda)
      on collective AI behavior.
    source: academic
    priority: 7
    subtopics: ["risk-management", "ai-safety", "multi-agent"]
    read: false
    notes_file: "../../link-dumps/notes/arxiv-2512.16856.md"
    added: "2025-12-31"
    tags: ["distributional-safety", "multi-agent-systems", "collective-risks", "sandbox-economies"]
    key_findings:
      - "Safety must address coordinated agent groups, not just individual systems"
      - "Proposes sandbox economies with market mechanisms for safety"
      - "Shift from single-system to distributed AI safety paradigm"

  - key: herrmann2025decision
    title: "A Decision-Theoretic Approach for Managing Misalignment"
    authors: ["Herrmann, Daniel A.", "Chari, Abinav", "Qian, Isabelle", "Sharvesh, Sree", "Levinstein, B. A."]
    year: 2025
    month: 12
    url: "https://arxiv.org/abs/2512.15584"
    arxiv: "https://arxiv.org/abs/2512.15584"
    abstract: |
      Decision-theoretic framework for AI delegation under misalignment. KEY INSIGHT: Don't pursue
      perfect alignment - balance value alignment, epistemic accuracy, decision-making reach.
      Distinguishes: (1) Universal delegation requires near-perfect alignment (rarely achievable),
      (2) Context-specific delegation can be optimal despite meaningful misalignment when agent offers
      superior accuracy or expanded capabilities. Introduces scoring methodology for trade-off evaluation
      under uncertainty. PRACTICAL IMPLICATION: Focus on "sufficiently aligned for particular applications"
      not "perfectly aligned." Relevant for Part 3 (framework) evaluation criteria, Part 4 (guidance)
      on acceptable alignment levels for different acquisition contexts, Part 5 (T&E) risk acceptance.
    source: academic
    priority: 7
    subtopics: ["risk-management", "ai-safety", "decision-theory", "evaluation-methodology"]
    read: false
    notes_file: "../../link-dumps/notes/arxiv-2512.15584.md"
    added: "2025-12-31"
    tags: ["decision-theory", "context-specific-delegation", "alignment-trade-offs", "practical-framework"]
    key_findings:
      - "Context-specific delegation can work despite misalignment"
      - "Balance alignment, accuracy, and capability reach"
      - "Focus on 'sufficient' not 'perfect' alignment for applications"
      - "Scoring methodology for delegation decisions under uncertainty"
