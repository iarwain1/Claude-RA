# Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon

**arXiv:** [2406.17746](https://arxiv.org/abs/2406.17746)
**Authors:** USVSN Sai Prashanth, Alvin Deng, Kyle O'Brien, Jyothir S, Mohammad Aflah Khan, et al. (12 total)
**Date:** 2024-06-25
**Categories:** cs.CL, cs.AI

## Abstract

Memorization in language models is typically treated as a homogenous phenomenon, neglecting the specifics of the memorized data. We instead model memorization as the effect of a set of complex factors that describe each sample and relate it to the model and corpus. To build intuition around these factors, we break memorization down into a taxonomy: recitation of highly duplicated sequences, reconstruction of inherently predictable sequences, and recollection of sequences that are neither. We demonstrate the usefulness of our taxonomy by using it to construct a predictive model for memorization. By analyzing dependencies and inspecting the weights of the predictive model, we find that different factors influence the likelihood of memorization differently depending on the taxonomic category.

---
*Metadata fetched via arxiv API on 2025-12-31*
