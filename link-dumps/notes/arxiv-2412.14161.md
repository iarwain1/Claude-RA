# TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks

**arXiv:** https://arxiv.org/abs/2412.14161
**Authors:** Frank F. Xu, Yufan Song, Boxuan Li, Yuxuan Tang, et al. (21 authors)
**Date:** 2024-12-18 (v1), revised 2025-09-10 (v3)
**GitHub:** https://github.com/TheAgentCompany/TheAgentCompany

## Abstract

TheAgentCompany is an extensible benchmark for evaluating AI agents that interact with the world like digital workers: browsing the web, writing code, running programs, and communicating with coworkers.

## Benchmark Features

- **Reproducible, self-hosted environment**
- **Simulated colleagues** to test agent communication
- **Checkpoint and execution-based evaluation**
- **175 diverse, realistic professional tasks** in a software company setting

## Research Question

How performant are AI agents at accelerating or autonomously performing work-related tasks? Important for:
- Industry looking to adopt AI into workflows
- Economic policy on labor market effects

## Key Findings

**Performance:**
- Best agent: **24% task completion rate**
- Shows promise in basic tasks
- Significant gaps vs. human performance in complex scenarios

**Implications:**
- AI agents could handle routine business tasks
- Substantial improvement needed for sophisticated work
- Current agents far from autonomous digital workers

## Claude Summary

TheAgentCompany provides a realistic benchmark for agent capabilities:

**Why this matters**:
- Unlike toy benchmarks, tasks reflect actual work
- Communication with simulated colleagues tests social intelligence
- Real-world consequences of actions are evaluated

**Key insight**: 24% autonomous completion is meaningful progress but highlights how far we are from AI doing knowledge work independently.

**Practical implications**:
- Useful for measuring progress toward autonomous agents
- Identifies specific capability gaps
- Informs expectations for AI-assisted workflows

## Relevance

Important benchmark for agent research. Provides realistic evaluation setting unlike many simpler benchmarks. Useful for understanding current capabilities and gaps in AI agents for work tasks.
