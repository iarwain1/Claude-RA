# Measuring what Matters: Construct Validity in Large Language Model Benchmarks

**arXiv:** https://arxiv.org/abs/2511.04703
**Authors:** Andrew M. Bean, Ryan Othniel Kearns, Angelika Romanou, Franziska Sofia Hafner, Harry Mayne, Jan Batzner, Negar Foroutan, Chris Schmitz, Karolina Korgul, Hunar Batra, Oishi Deb, Emma Beharry, Cornelius Emde, Thomas Foster, Anna Gausen, Mar√≠a Grandury, Simeng Han, Valentin Hofmann, Lujain Ibrahim, Hazel Kim, Hannah Rose Kirk, Fangru Lin, and 20+ other authors
**Date:** 2025-11-03

## Abstract

Evaluating large language models (LLMs) is crucial for both assessing their capabilities and identifying safety or robustness issues prior to deployment. Reliably measuring abstract and complex phenomena such as 'safety' and 'robustness' requires strong construct validity, that is, having measures that represent what matters to the phenomenon.

With a team of 29 expert reviewers, the authors conducted a systematic review of 445 LLM benchmarks from leading conferences in natural language processing and machine learning. Across the reviewed articles, they find patterns related to the measured phenomena, tasks, and scoring metrics which undermine the validity of the resulting claims.

To address these shortcomings, they provide eight key recommendations and detailed actionable guidance to researchers and practitioners in developing LLM benchmarks.

## Claude Summary

This is a landmark systematic review of LLM benchmark validity with 29 expert reviewers examining 445 benchmarks. The paper identifies pervasive issues with construct validity - the benchmarks often don't actually measure what they claim to measure.

Key findings indicate patterns that undermine validity:
- Disconnect between claimed phenomena (safety, robustness) and actual tasks
- Scoring metrics that don't capture the construct of interest
- Missing validation of whether benchmarks measure intended attributes

The 8 recommendations provide actionable guidance for developing more valid benchmarks, making this essential reading for anyone designing or using LLM evaluations.

## Relevance

Highly relevant for evaluation methodology. Directly addresses fundamental questions about what AI benchmarks actually measure, which is critical for AI safety assessment and governance.
