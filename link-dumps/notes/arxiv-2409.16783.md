# Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction

**arXiv:** [2409.16783](https://arxiv.org/abs/2409.16783)
**Authors:** Jinchuan Zhang, Yan Zhou, Yaxin Liu, Ziming Li, Songlin Hu
**Date:** 2024-09-25
**Categories:** cs.CL, cs.AI, cs.CR

## Abstract

Automated red teaming is an effective method for identifying misaligned behaviors in large language models (LLMs). Existing approaches, however, often focus primarily on improving attack success rates while overlooking the need for comprehensive test case coverage. Additionally, most of these methods are limited to single-turn red teaming, failing to capture the multi-turn dynamics of real-world human-machine interactions. To overcome these limitations, we propose HARM (Holistic Automated Red teaMing), which scales up the diversity of test cases using a top-down approach based on an extensible, fine-grained risk taxonomy. Our method also leverages a novel fine-tuning strategy and reinforcement learning techniques to facilitate multi-turn adversarial probing in a human-like manner. Experimental results demonstrate that our framework enables a more systematic understanding of model vulnerabilities and offers more targeted guidance for the alignment process.

---
*Metadata fetched via arxiv API on 2025-12-31*
