# Does Refusal Training in LLMs Generalize to the Past Tense?

**arXiv:** [2407.11969](https://arxiv.org/abs/2407.11969)
**Authors:** Maksym Andriushchenko, Nicolas Flammarion
**Date:** 2024-07-16

## Abstract

The authors reveal a curious generalization gap in current refusal training: simply reformulating a harmful request in the past tense is often sufficient to jailbreak many state-of-the-art LLMs. For GPT-4o, the success rate increases from 1% using direct requests to 88% using 20 past tense reformulation attempts. The findings highlight that widely used alignment techniques can be brittle.
