# Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming

**arXiv:** https://arxiv.org/abs/2501.18837
**Authors:** Mrinank Sharma et al. (43 authors, Anthropic)
**Date:** 2025-01-31
**Blog:** https://www.anthropic.com/research/constitutional-classifiers

## Abstract

LLMs are vulnerable to **universal jailbreaks** - prompting strategies that systematically bypass safeguards and enable harmful processes requiring many model interactions (e.g., manufacturing illegal substances at scale).

**Constitutional Classifiers** are safeguards trained on synthetic data generated by prompting LLMs with natural language rules (a "constitution") specifying permitted and restricted content.

## Key Results

**Red Teaming Campaign:**
- 183 participants
- 3,000+ estimated hours over two months
- Up to $15,000 reward for finding universal jailbreak
- **No universal jailbreak found** against classifier-guarded LLM

**Effectiveness:**
- Baseline (no protection): 86% jailbreak success
- With Constitutional Classifiers: **4.4% jailbreak success**
- Dramatic reduction in vulnerability

**Costs:**
- False positive refusals increased by 0.38%
- Computational cost increased by 23.7%
- Acceptable trade-offs for security improvement

## Technical Approach

- Classifier safeguards monitor inputs and outputs
- Constitution defines harmful/harmless content categories
- Synthetic data generation enables rapid adaptation
- Can update to new threat models by modifying constitution

## Claude Summary

Constitutional Classifiers represent a significant defense advance:

**Why this matters**:
- Universal jailbreaks are especially concerning (enable scaled harm)
- This defense dramatically reduces success rate
- Empirically validated through extensive adversarial testing

**Key innovation**: Using constitutional AI principles for classifier training. The constitution enables:
- Rapid adaptation to new threats
- Principled coverage of harm categories
- Synthetic data generation at scale

**Limitations to consider**:
- 4.4% still not zero
- Computational overhead may be prohibitive for some applications
- Adversaries may eventually find new attack vectors

**Part of RSP**: Developed under Anthropic's Responsible Scaling Policy commitments.

## Relevance

**Critical for AI safety.** State-of-the-art defense against jailbreaks from a leading safety lab. Demonstrates that significant security improvements are possible with acceptable costs.
