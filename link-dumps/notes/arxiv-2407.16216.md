# A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO, DPO and More

**arXiv:** [2407.16216](https://arxiv.org/abs/2407.16216)
**Authors:** Zhichao Wang, Bin Bi, Shiva Kumar Pentyala, Kiran Ramnath, et al.
**Date:** 2024-07-23

## Abstract

Various methods have been proposed to enhance LLMs, particularly in aligning them with human expectation. This work categorizes these papers into distinct topics and provides detailed explanations of each alignment method. The survey covers Reward Models, Feedback, Reinforcement Learning, and Optimization techniques.
