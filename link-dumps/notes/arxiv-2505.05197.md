# Societal and technological progress as sewing an ever-growing, ever-changing, patchy, and polychrome quilt

**arXiv:** [2505.05197](https://arxiv.org/abs/2505.05197)
**Authors:** Joel Z. Leibo, Alexander Sasha Vezhnevets, William A. Cunningham, SÃ©bastien Krier, Manfred Diaz, et al. (6 total)
**Date:** 2025-05-08
**Categories:** cs.AI, cs.CY

## Abstract

Artificial Intelligence (AI) systems are increasingly placed in positions where their decisions have real consequences, e.g., moderating online spaces, conducting research, and advising on policy. Ensuring they operate in a safe and ethically acceptable fashion is thus critical. However, most solutions have been a form of one-size-fits-all "alignment". We are worried that such systems, which overlook enduring moral diversity, will spark resistance, erode trust, and destabilize our institutions. This paper traces the underlying problem to an often-unstated Axiom of Rational Convergence: the idea that under ideal conditions, rational agents will converge in the limit of conversation on a single ethics. Treating that premise as both optional and doubtful, we propose what we call the appropriateness framework: an alternative approach grounded in conflict theory, cultural evolution, multi-agent systems, and institutional economics. The appropriateness framework treats persistent disagreement as the normal case and designs for it by applying four principles: (1) contextual grounding, (2) community customization, (3) continual adaptation, and (4) polycentric governance. We argue here that adopting these design principles is a good way to shift the main alignment metaphor from moral unification to a more productive metaphor of conflict management, and that taking this step is both desirable and urgent.

---
*Metadata fetched via arxiv API on 2025-12-31*
