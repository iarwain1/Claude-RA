# Stress-Testing Capability Elicitation With Password-Locked Models

**arXiv:** [2405.19550](https://arxiv.org/abs/2405.19550)
**Authors:** Ryan Greenblatt, Fabien Roger, Dmitrii Krasheninnikov, David Krueger
**Date:** 2024-05-29
**Code:** [github.com/FabienRoger/sandbagging](https://github.com/FabienRoger/sandbagging)

## Abstract

To determine the safety of large language models (LLMs), AI developers must be able to assess their dangerous capabilities. But simple prompting strategies often fail to elicit an LLM's full capabilities. One way to elicit capabilities more robustly is to fine-tune the LLM to complete the task.

In this paper, we investigate the conditions under which fine-tuning-based elicitation suffices to elicit capabilities. To do this, we introduce password-locked modelsâ€”LLMs fine-tuned such that some of their capabilities are deliberately hidden. Specifically, these LLMs are trained to exhibit these capabilities only when a password is present in the prompt, and to imitate a much weaker LLM otherwise.

We find that a few high-quality demonstrations are often sufficient to fully elicit password-locked capabilities. More surprisingly, fine-tuning can elicit other capabilities that have been locked using the same password, or even different passwords. Furthermore, when only evaluations, and not demonstrations, are available, approaches like reinforcement learning are still often able to elicit capabilities.

## Claude Summary

This paper is foundational for understanding sandbagging detection. The key insight is that models can be deliberately trained to hide capabilities (password-locking), but these capabilities can often be recovered through fine-tuning. This is important for AI safety because:

1. It shows fine-tuning is an effective elicitation method for current models
2. It reveals limitations when high-quality demonstrations aren't available
3. It suggests methods may be unreliable when model capabilities exceed human demonstrators

**Relevance to evaluations:** Essential reading for anyone working on capability elicitation or sandbagging detection.
