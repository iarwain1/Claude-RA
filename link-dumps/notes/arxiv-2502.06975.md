# Position: Episodic Memory is the Missing Piece for Long-Term LLM Agents

**arXiv:** [2502.06975](https://arxiv.org/abs/2502.06975)
**Authors:** Mathis Pink, Qinyuan Wu, Vy Ai Vo, Javier Turek, Jianing Mu, et al. (7 total)
**Date:** 2025-02-10
**Categories:** cs.AI

## Abstract

As Large Language Models (LLMs) evolve from text-completion tools into fully fledged agents operating in dynamic environments, they must address the challenge of continually learning and retaining long-term knowledge. Many biological systems solve these challenges with episodic memory, which supports single-shot learning of instance-specific contexts. Inspired by this, we present an episodic memory framework for LLM agents, centered around five key properties of episodic memory that underlie adaptive and context-sensitive behavior. With various research efforts already partially covering these properties, this position paper argues that now is the right time for an explicit, integrated focus on episodic memory to catalyze the development of long-term agents. To this end, we outline a roadmap that unites several research directions under the goal to support all five properties of episodic memory for more efficient long-term LLM agents.

---
*Metadata fetched via arxiv API on 2025-12-31*
