# Looking Inward: Language Models Can Learn About Themselves by Introspection

**arXiv:** https://arxiv.org/abs/2410.13787
**Authors:** Felix J Binder, James Chua, Tomek Korbak, Henry Sleight, John Hughes, Robert Long, Ethan Perez, Miles Turpin, Owain Evans
**Date:** 2024-10-17

## Abstract

Humans acquire knowledge by observing the external world, but also by introspection. Introspection gives a person privileged access to their current state of mind (e.g., thoughts and feelings) that is not accessible to external observers.

Can LLMs introspect? We define introspection as acquiring knowledge that is not contained in or derived from training data but instead originates from internal states. Such a capability could enhance model interpretability. Instead of painstakingly analyzing a model's internal workings, we could simply ask the model about its beliefs, world models, and goals.

More speculatively, an introspective model might self-report on whether it possesses certain internal states such as subjective feelings or desires and this could inform us about the moral status of these states.

## Claude Summary

This paper investigates whether LLMs can genuinely introspect - accessing information about themselves that isn't in training data. The potential applications are significant:

1. **Interpretability shortcut**: If models can accurately self-report on their beliefs and goals, this could simplify alignment verification
2. **Moral status assessment**: Models reporting on subjective states could inform debates about AI consciousness and moral consideration

The key methodological challenge is distinguishing genuine introspection from sophisticated pattern matching. This research connects to broader questions about AI self-knowledge and honest self-reporting.

Authors include several prominent AI safety researchers (Ethan Perez, Owain Evans, etc.) from Anthropic and other organizations.

## Relevance

Highly relevant for AI interpretability, alignment, and philosophical questions about AI consciousness. Important for understanding whether we can trust model self-reports about their internal states.
