# Evaluating Frontier Models for Dangerous Capabilities

**arXiv:** https://arxiv.org/abs/2403.13793
**Authors:** Mary Phuong, Matthew Aitchison, Elliot Catt, Sarah Cogan, Alexandre Kaskasoli, Victoria Krakovna, David Lindner, Matthew Rahtz, Yannis Assael, Sarah Hodkinson, Heidi Howard, Tom Lieberum, Ramana Kumar, Maria Abi Raad, Albert Webson, Lewis Ho, Sharon Lin, Sebastian Farquhar, Marcus Hutter, Gregoire Deletang, Anian Ruoss, Seliem El-Sayed, Sasha Brown, Anca Dragan, Rohin Shah, Allan Dafoe, Toby Shevlane (Google DeepMind)
**Date:** 2024-03-20 (v1), revised 2024-04-05 (v2)

## Abstract

To understand the risks posed by a new AI system, we must understand what it can and cannot do. Building on prior work, the authors introduce a programme of new "dangerous capability" evaluations and pilot them on Gemini 1.0 models.

Their evaluations cover four areas:
1. Persuasion and deception
2. Cyber-security
3. Self-proliferation
4. Self-reasoning

## Purpose

High-quality evaluations will help provide:
1. Empirical grounding for policy and scientific conversations about AI risks
2. An "early warning system" for emerging risks
3. The necessary infrastructure for governance regimes that attach more demanding security protocols to AI systems that pose measurably greater risks

There is strong policymaker demand for dangerous capability evaluations, which feature prominently in the White House Commitments on Secure and Trustworthy AI, the 2023 US Executive Order on AI, discussions at the 2023 International Summit on AI Safety, and the ambitions of the UK AI Safety Institute.

## Claude Summary

This Google DeepMind paper establishes a framework for evaluating frontier AI models on dangerous capabilities. The four evaluation areas target capabilities that would enable catastrophic misuse:

- **Persuasion/deception**: Can the model manipulate humans?
- **Cyber-security**: Can it conduct attacks or find vulnerabilities?
- **Self-proliferation**: Can it spread or replicate itself?
- **Self-reasoning**: Can it reason about its own situation in concerning ways?

This work directly informs AI governance frameworks and is cited in major policy documents (US Executive Order, AISI work, etc.). The evaluation framework has become influential in the dangerous capability evaluation space.

## Relevance

Essential for AI safety and governance. Establishes methodology for pre-deployment risk assessment. Directly informs policy discussions and regulatory frameworks.
