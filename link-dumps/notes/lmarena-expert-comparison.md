# Arena Expert Model Comparison

**URL:** https://news.lmarena.ai/arena-expert-model-comparison/
**Publisher:** LMArena
**Date:** 2025-12 (data from December 1, 2025)
**Type:** Benchmark Study

## Summary

Comprehensive analysis of how 139 frontier AI models perform with expert users compared to general users, revealing significant performance differences based on model architecture and company.

## Models Analyzed

- **Total models:** 139 frontier models (ELO rating 1300+)
- **Breakdown:** 42 thinking models, 97 non-thinking models
- **Companies:** Anthropic, OpenAI, Google, xAI, Alibaba, DeepSeek

## Key Findings

### Thinking vs. Non-Thinking Models

**Overall Trend:**
- Thinking models: +15 median expert advantage
- Non-thinking models: -9 median expert advantage
- **Gap:** 24-point difference in expert preference

**Exception to the Rule:**
Performance varies significantlyâ€”architecture alone doesn't determine expert preference.

### Top Performers

**Absolute Best:**
- **Claude Opus 4.5 (non-thinking):** +85 expert advantage (highest of any model)
- **Claude Sonnet 4.5 Thinking:** +57

**Underperformers:**
- o1-preview (thinking): -11 (despite being a thinking model)

### Company Rankings (Average Expert Advantage)

1. **Anthropic:** +22
2. **Alibaba:** +14
3. **OpenAI:** -3 (near neutral)
4. **Google:** Negative
5. **DeepSeek:** Negative

## Methodology

**Data Source:** LMArena platform (December 1, 2025)

**Approach:**
- Style control filtering
- Minimum performance thresholds
- Comparison of general user preferences vs. expert-only prompts

**Metric:** "Expert Advantage" = performance differential between expert and general user segments

## Implications

**Architecture Matters, But Isn't Everything:**
Claude Opus 4.5 (non-thinking) outperforms many thinking models with experts.

**Company-Level Differences:**
Significant variation in how well different companies' models serve expert users, with Anthropic leading by substantial margin.

**Use-Case Specificity:**
Expert preferences differ markedly from general users, suggesting models optimize for different user segments.

## Relevance

**Important for model selection and evaluation.** Shows that expert user preferences diverge significantly from general benchmarks. Challenges assumptions about thinking vs. non-thinking architectures. Demonstrates company-level quality differences.

---
*Metadata fetched via WebFetch on 2025-12-31*
