# From LLMs to LLM-based Agents for Software Engineering: A Survey

**arXiv:** https://arxiv.org/abs/2408.02479
**Authors:** Haolin Jin, Linghan Huang, Haipeng Cai, Jun Yan, Bo Li, Huaming Chen
**Date:** 2024-08-05 (v1), revised 2025-04-13 (v2)

## Abstract

With the rise of large language models (LLMs), researchers are increasingly exploring their applications in various vertical domains, such as software engineering. LLMs have achieved remarkable success in areas including code generation and vulnerability detection. However, they also exhibit numerous limitations and shortcomings.

LLM-based agents, a novel technology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement. Despite numerous studies and surveys exploring the possibility of using LLMs in software engineering, it lacks a clear distinction between LLMs and LLM-based agents. It is still in its early stage for a unified standard and benchmarking to qualify an LLM solution as an LLM-based agent in its domain.

## Claude Summary

This survey distinguishes between using LLMs directly for software engineering tasks versus using LLM-based agents (autonomous systems that use LLMs for decision-making). The key difference: agents can take actions, use tools, and potentially self-improve, while raw LLMs are limited to text generation.

The survey covers applications in code generation, vulnerability detection, testing, and other SE domains. It identifies that the field lacks unified standards for what qualifies as an "agent" versus just an "LLM application."

Important for understanding the evolution from static LLM tools to autonomous coding agents like Devin, Claude Code, etc.

## Relevance

Relevant for understanding AI agents in software development context. Useful taxonomy for distinguishing capability levels. Important for agent evaluation discussions.
