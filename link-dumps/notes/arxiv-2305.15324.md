# Model Evaluation for Extreme Risks

**arXiv:** https://arxiv.org/abs/2305.15324
**Authors:** Toby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittlestone, Jade Leung, Daniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, Lewis Ho, Divya Siddarth, Shahar Avin, Will Hawkins, Been Kim, Iason Gabriel, Vijay Bolina, Jack Clark, Yoshua Bengio, Paul Christiano, Allan Dafoe
**Organizations:** ARC, Anthropic, DeepMind, Cambridge, OpenAI, GovAI, and others
**Date:** 2023-05-24

## Abstract

Current approaches to building general-purpose AI systems tend to produce systems with both beneficial and harmful capabilities. Further progress in AI development could lead to capabilities that pose extreme risks, such as offensive cyber capabilities or strong manipulation skills.

The paper explains why model evaluation is critical for addressing extreme risks. Developers must be able to identify:
1. **Dangerous capabilities** (through "dangerous capability evaluations")
2. **Propensity to apply capabilities for harm** (through "alignment evaluations")

These evaluations will become critical for keeping policymakers and other stakeholders informed, and for making responsible decisions about model training, deployment, and security.

## Claude Summary

This is a foundational multi-stakeholder paper establishing the framework for evaluating AI models for extreme/catastrophic risks. Co-authored by researchers across major AI labs and policy organizations.

**Key framework distinction:**
- **Dangerous capability evaluations**: Can the model do harmful things?
- **Alignment evaluations**: Will the model do harmful things?

**Risk categories addressed:**
- Offensive cyber capabilities
- Manipulation/persuasion abilities
- Autonomous replication/acquisition of resources
- Biological/chemical weapon assistance

**Importance**: This paper shaped subsequent work on frontier model evaluation, including the UK AISI approach and various company pre-deployment evaluation protocols.

## Relevance

**Foundational for AI safety evaluation.** Establishes the evaluation paradigm used by AI labs and safety institutes. Essential reading for understanding how dangerous capability assessments are conceptualized and conducted.
