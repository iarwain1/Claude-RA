# Intolerable Risk Threshold Recommendations for Artificial Intelligence

**arXiv:** https://arxiv.org/abs/2503.05812
**Authors:** Deepika Raman, Nada Madkour, Evan R. Murphy, Krystal Jackson, Jessica Newman (UC Berkeley CLTC)
**Date:** 2025-03-04
**CLTC:** https://cltc.berkeley.edu/publication/intolerable-ai-risk-thresholds/

## Abstract

Frontier AI models may pose severe risks to public safety, human rights, economic stability, and societal value. These risks could arise from misuse, system failures, unintended effects, or simultaneous multi-model failures.

## Context: Seoul Summit Commitments

At AI Seoul Summit (May 2024):
- 16 AI organizations signed Frontier AI Safety Commitments
- 27 nations + EU declared intent to define risk thresholds
- Organizations must determine "thresholds at which severe risks... would be deemed intolerable"

## Key Principles

**"Good, not perfect" thresholds:**
- Limited data on rapidly advancing capabilities
- Risks evolve with technology
- Perfect thresholds impossible; actionable ones essential

## Eight Risk Categories with Specific Recommendations

1. **CBRN Weapons** - Chemical, Biological, Radiological, Nuclear
2. **Cyber Attacks**
3. **Model Autonomy**
4. **Persuasion and Manipulation**
5. **Deception**
6. **Toxicity**
7. **Discrimination**
8. **Socioeconomic Disruption**

Each category includes detailed case studies and threshold recommendations.

## Purpose

**Starting point for:**
- Policymakers
- Industry leaders

**Goal:** Proactive risk management prioritizing prevention (ex ante) over mitigation (ex post).

## Claude Summary

This paper operationalizes the concept of "intolerable risk":

**Why thresholds matter**: Without specific thresholds, "unacceptable risk" is vague. This paper provides concrete recommendations.

**Eight categories**: Comprehensive coverage of major risk types. Each with specific, actionable thresholds.

**From UC Berkeley CLTC**: Credible source with policy expertise. Designed to be useful for both regulators and industry.

**Connection to Seoul Commitments**: Directly supports fulfilling international commitments. Practical guidance for compliance.

## Relevance

**Essential for AI governance.** Provides specific threshold recommendations for major risk categories. Supports implementation of Frontier AI Safety Commitments. From UC Berkeley Center for Long-Term Cybersecurity.
