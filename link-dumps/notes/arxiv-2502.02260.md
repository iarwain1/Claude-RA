# Adversarial ML Problems Are Getting Harder to Solve and to Evaluate

**arXiv:** [2502.02260](https://arxiv.org/abs/2502.02260)
**Authors:** Javier Rando, Jie Zhang, Nicholas Carlini, Florian Tram√®r
**Date:** 2025-02-04
**Categories:** cs.LG, cs.CR

## Abstract

In the past decade, considerable research effort has been devoted to securing machine learning (ML) models that operate in adversarial settings. Yet, progress has been slow even for simple "toy" problems (e.g., robustness to small adversarial perturbations) and is often hindered by non-rigorous evaluations. Today, adversarial ML research has shifted towards studying larger, general-purpose language models. In this position paper, we argue that the situation is now even worse: in the era of LLMs, the field of adversarial ML studies problems that are (1) less clearly defined, (2) harder to solve, and (3) even more challenging to evaluate. As a result, we caution that yet another decade of work on adversarial ML may fail to produce meaningful progress.

---
*Metadata fetched via arxiv API on 2025-12-31*
