# Position: Evaluating Generative AI Systems Is a Social Science Measurement Challenge

**arXiv:** https://arxiv.org/abs/2502.00561
**Authors:** Hanna Wallach, Meera Desai, A. Feder Cooper, et al. (20 authors, Microsoft Research)
**Date:** 2025-02-01 (v1), revised 2025-06-06 (v2)
**Venue:** ICML 2025 (PMLR)

## Abstract

The measurement tasks involved in evaluating GenAI systems lack sufficient scientific rigor, leading to "a tangle of sloppy tests [and] apples-to-oranges comparisons."

**Position:** Evaluating GenAI systems is a social science measurement challenge. The ML community would benefit from learning from social sciences when developing measurement instruments.

## Four-Level Framework

Grounded in measurement theory from social sciences, for measuring:
- Capabilities of GenAI systems
- Behaviors of GenAI systems
- Impacts of GenAI systems

## Key Implications

**1. Broadens expertise:**
- Enables stakeholders with different perspectives to participate
- Conceptual debates can include domain experts, not just ML researchers

**2. Improves rigor:**
- Measurement theory provides established frameworks
- Can identify construct validity issues in current evaluations

## The Problem with Current Evaluations

- Sloppy operationalization of concepts
- Unclear what benchmarks actually measure
- Difficulty comparing across different evaluations
- Missing validity and reliability assessments

## Claude Summary

This paper argues that AI evaluation needs social science methodology:

**The core insight**: We're trying to measure complex constructs (capabilities, safety, etc.) but using ad-hoc methods. Social science has developed rigorous approaches to this over decades.

**Why this matters for AI safety**:
- If we can't properly measure safety, we can't ensure it
- Construct validity problems mean benchmarks may not measure what we think
- Better methodology leads to better evaluations

**From Microsoft Research**: Credible source arguing for more rigor in evaluation. Not anti-benchmark, but pro-better-benchmarks.

## Relevance

**Important for evaluation methodology.** Provides principled framework for thinking about what evaluations actually measure. Relevant for anyone designing or interpreting AI evaluations.
