# Are language models rational? The case of coherence norms and belief revision

**arXiv:** [2406.03442](https://arxiv.org/abs/2406.03442)
**Authors:** Thomas Hofweber, Peter Hase, Elias Stengel-Eskin, Mohit Bansal
**Date:** 2024-06-05
**Categories:** cs.CL, cs.AI

## Abstract

Do norms of rationality apply to machine learning models, in particular language models? In this paper we investigate this question by focusing on a special subset of rational norms: coherence norms. We consider both logical coherence norms as well as coherence norms tied to the strength of belief. To make sense of the latter, we introduce the Minimal Assent Connection (MAC) and propose a new account of credence, which captures the strength of belief in language models. This proposal uniformly assigns strength of belief simply on the basis of model internal next token probabilities. We argue that rational norms tied to coherence do apply to some language models, but not to others. This issue is significant since rationality is closely tied to predicting and explaining behavior, and thus it is connected to considerations about AI safety and alignment, as well as understanding model behavior more generally.

---
*Metadata fetched via arxiv API on 2025-12-31*
