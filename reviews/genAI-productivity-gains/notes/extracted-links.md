# Categorized Links from Dump Files

Total unique links: 1623

## High-Priority Sources

### METR (Model Evaluation and Threat Research)
http://metr.org/faisc
https://metr.org/blog/2024-08-06-update-on-evaluations/
https://metr.org/blog/2024-08-29-common-elements-of-frontier-ai-safety-policies/
https://metr.org/blog/2024-11-12-rogue-replication-threat-model/
https://metr.org/blog/2024-11-22-evaluating-r-d-capabilities-of-llms/
https://metr.org/blog/2025-01-17-ai-models-dangerous-before-public-deployment/
https://metr.org/blog/2025-02-14-measuring-automated-kernel-engineering/
https://metr.org/blog/2025-06-05-recent-reward-hacking/
https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/
https://metr.org/blog/2025-07-14-how-does-time-horizon-vary-across-domains/
https://metr.org/blog/2025-08-12-research-update-towards-reconciling-slowdown-with-time-horizons/
https://metr.org/hcast.pdf

### OpenAI
http://cdn.openai.com/papers/openais-approach-to-external-red-teaming.pdf
https://alignment.openai.com/scaling-code-verification/
https://cdn.openai.com/11998be9-5319-4302-bfbf-1167e093f1fb/Native_Image_Generation_System_Card.pdf
https://cdn.openai.com/API/docs/gpt-5-for-coding-cheatsheet.pdf
https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf
https://cdn.openai.com/business-guides-and-resources/building-an-ai-native-engineering-team.pdf
https://cdn.openai.com/operator_system_card.pdf
https://cdn.openai.com/pdf/6a2631dc-783e-479b-b1a4-af0cfbd38630/how-openai-uses-codex.pdf
https://cookbook.openai.com/examples/file_search_responses
https://cookbook.openai.com/examples/gpt-5/gpt-5-1-codex-max_prompting_guide
https://cookbook.openai.com/examples/gpt-5/gpt-5-1_prompting_guide
https://developers.openai.com/blog/what-makes-a-great-chatgpt-app
https://openai.com/global-affairs/openais-economic-blueprint/
https://openai.com/index/accelerating-science-gpt-5/
https://openai.com/index/advancing-red-teaming-with-people-and-ai/
https://openai.com/index/chain-of-thought-monitoring/
https://openai.com/index/chatgpt-agent-system-card/
https://openai.com/index/deep-research-system-card/
https://openai.com/index/detecting-and-reducing-scheming-in-ai-models/
https://openai.com/index/extracting-concepts-from-gpt-4/
https://openai.com/index/finding-gpt4s-mistakes-with-gpt-4/
https://openai.com/index/gdpval/
https://openai.com/index/gpt-4o-system-card/
https://openai.com/index/how-confessions-can-keep-language-models-honest/
https://openai.com/index/improving-model-safety-behavior-with-rule-based-rewards/
https://openai.com/index/introducing-o3-and-o4-mini/
https://openai.com/index/introducing-swe-bench-verified/
https://openai.com/index/o3-o4-mini-system-card/
https://openai.com/index/prover-verifier-games-improve-legibility/
https://openai.com/index/sharing-the-latest-model-spec/
https://openai.com/index/swe-lancer/
https://openai.com/index/teaching-models-to-express-their-uncertainty-in-words/
https://openai.com/index/teen-safety-freedom-and-privacy/
https://openai.com/index/the-state-of-enterprise-ai-2025-report/
https://openai.com/index/trading-inference-time-compute-for-adversarial-robustness/
https://openai.com/index/understanding-neural-networks-through-sparse-circuits/
https://openai.com/index/updating-our-preparedness-framework/
https://openai.com/safety/how-we-think-about-safety-alignment/
https://platform.openai.com/docs/guides/optimizing-llm-accuracy
https://platform.openai.com/docs/guides/reasoning-best-practices

### Anthropic
http://red.anthropic.com/
https://alignment.anthropic.com/2025/automated-auditing/
https://alignment.anthropic.com/2025/automated-researchers-sandbag/
https://alignment.anthropic.com/2025/bumpers/
https://alignment.anthropic.com/2025/honesty-elicitation/
https://alignment.anthropic.com/2025/recommended-directions/
https://alignment.anthropic.com/2025/sabotage-risk-report/
https://alignment.anthropic.com/2025/stress-testing-model-specs/
https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf
https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf
https://docs.anthropic.com/llms-full.txt
https://www-cdn.anthropic.com/58284b19e702b49db9302d5b6f135ad8871e7658.pdf
https://www.anthropic.com/engineering
https://www.anthropic.com/engineering/built-multi-agent-research-system
https://www.anthropic.com/engineering/claude-code-best-practices
https://www.anthropic.com/engineering/claude-think-tool
https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents
https://www.anthropic.com/news/a-new-initiative-for-developing-third-party-model-evaluations
https://www.anthropic.com/news/how-people-use-claude-for-support-advice-and-companionship
https://www.anthropic.com/news/language-models-mostly-know-what-they-know
https://www.anthropic.com/news/model-context-protocol
https://www.anthropic.com/news/model-safety-bug-bounty
https://www.anthropic.com/news/our-approach-to-understanding-and-addressing-ai-harms
https://www.anthropic.com/news/paris-ai-summit
https://www.anthropic.com/news/research
https://www.anthropic.com/news/the-anthropic-economic-index
https://www.anthropic.com/news/the-need-for-transparency-in-frontier-ai
https://www.anthropic.com/research/agentic-misalignment
https://www.anthropic.com/research/alignment-faking
https://www.anthropic.com/research/auditing-hidden-objectives
https://www.anthropic.com/research/building-effective-agents
https://www.anthropic.com/research/constitutional-classifiers
https://www.anthropic.com/research/emergent-misalignment-reward-hacking
https://www.anthropic.com/research/forecasting-rare-behaviors
https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic
https://www.anthropic.com/research/introspection
https://www.anthropic.com/research/petri-open-source-auditing
https://www.anthropic.com/research/shade-arena-sabotage-monitoring
https://www.anthropic.com/research/statistical-approach-to-model-evals
https://www.anthropic.com/research/tracing-thoughts-language-model

### Epoch AI
http://epoch.ai/data/ai-benchmarking-dashboard
https://epoch.ai/benchmarks
https://epoch.ai/blog/what-does-osworld-tell-us-about-ais-ability-to-use-computers
https://epoch.ai/blog/what-skills-does-swe-bench-verified-evaluate
https://epoch.ai/blog/what-will-ai-look-like-in-2030
https://epoch.ai/data/ai-benchmarking-dashboard
https://epoch.ai/gradient-updates/benchmark-scores-general-capability-claudiness
https://epoch.ai/gradient-updates/quantifying-the-algorithmic-improvement-from-reasoning-models
https://epoch.ai/gradient-updates/the-real-reason-ai-benchmarks-havent-reflected-economic-impacts

### NBER
https://www.nber.org/books-and-chapters/economics-transformative-ai/coasean-singularity-demand-supply-and-market-design-ai-agents
https://www.nber.org/papers/w32980
https://www.nber.org/papers/w34256

### Productivity-Specific
https://openai.com/index/gdpval/
https://secondthoughts.ai/p/hyperproductivity
https://www.remotelabor.ai/
https://www.upwork.com/static/webflow/assets/webflow-human-agent-productivity-index/upbench_paper.pdf

### AI Safety and Evaluation Organizations
https://cdn.governance.ai/GovAI_Annual_Report_2023.pdf
https://cset.georgetown.edu/article/evaluating-large-language-models/
https://cset.georgetown.edu/article/mapping-the-ai-governance-landscape/
https://cset.georgetown.edu/publication/ai-for-military-decision-making/
https://cset.georgetown.edu/publication/ai-for-military-decision-making/?utm_source=Center%2Bfor%2BSecurity%2Band%2BEmerging%2BTechnology&amp;utm_campaign=6ae948a859-AI%2Bfor%2BMilitary%2BDecision-Making&amp;utm_medium=email&amp;utm_term=0_fcbacf8c3e-6ae948a859-438406133
https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-reliable-uncertainty-quantification-in-machine-learning/
https://cset.georgetown.edu/publication/through-the-chat-window-and-into-the-real-world-preparing-for-ai-agents/
https://www.aisi.gov.uk/research-agenda
https://www.aisi.gov.uk/research/understanding-ai-trajectories-mapping-the-limitations-of-current-ai-systems
https://www.aisi.gov.uk/work/early-insights-from-developing-question-answer-evaluations-for-frontier-ai
https://www.aisi.gov.uk/work/how-can-safety-cases-be-used-to-help-with-frontier-ai-safety
https://www.aisi.gov.uk/work/inspect-cyber
https://www.aisi.gov.uk/work/inspect-evals
https://www.aisi.gov.uk/work/llm-judges-on-trial-a-new-statistical-framework-to-assess-autograders
https://www.aisi.gov.uk/work/long-form-tasks
https://www.aisi.gov.uk/work/making-safeguard-evaluations-actionable
https://www.aisi.gov.uk/work/our-approach-to-ai-capability-elicitation
https://www.aisi.gov.uk/work/pre-deployment-evaluation-of-anthropics-upgraded-claude-3-5-sonnet
https://www.aisi.gov.uk/work/pre-deployment-evaluation-of-openais-o1-model
https://www.aisi.gov.uk/work/principles-for-safeguard-evaluation
https://www.aisi.gov.uk/work/replibench-measuring-autonomous-replication-capabilities-in-ai-systems
https://www.aisi.gov.uk/work/why-were-working-on-white-box-control
https://www.governance.ai/post/managing-risks-from-ai-enabled-biological-tools
https://www.governance.ai/post/predicting-ais-impact-on-work
https://www.governance.ai/post/the-case-for-supervising-frontier-ai-developers
https://www.governance.ai/research-paper/assessing-risk-relative-to-competitors-an-analysis-of-current-ai-company-policies
https://www.rand.org/pubs/perspectives/PEA3691-12.html
https://www.rand.org/pubs/perspectives/PEA3691-4.html
https://www.rand.org/pubs/perspectives/PEA4155-1.html
https://www.rand.org/pubs/perspectives/PEA4189-1.html
https://www.rand.org/pubs/perspectives/PEA4361-1.html
https://www.rand.org/pubs/research_briefs/RBA2981-1.html
https://www.rand.org/pubs/research_reports/RRA2981-1.html
https://www.rand.org/pubs/research_reports/RRA3034-1.html
https://www.rand.org/pubs/research_reports/RRA3034-2.html
https://www.rand.org/pubs/research_reports/RRA3408-1.html
https://www.rand.org/pubs/research_reports/RRA3572-1.html
https://www.rand.org/pubs/working_papers/WRA4005-1.html
https://www.rand.org/randeurope/research/projects/2024/ai-risk-index.html

### Key Bloggers
https://blog.samaltman.com/abundant-intelligence
https://helentoner.substack.com/p/2-big-questions-for-ai-progress-in
https://helentoner.substack.com/p/taking-jaggedness-seriously
https://helentoner.substack.com/p/unresolved-debates-about-the-future
https://thezvi.substack.com/p/on-dwarkesh-patels-4th-podcast-with
https://www.dwarkesh.com/p/questions-about-ai
https://www.dwarkesh.com/p/thoughts-on-the-ai-buildout
https://www.dwarkeshpatel.com/p/ai-firm
https://www.dwarkeshpatel.com/p/tyler-cowen-4
https://www.oneusefulthing.org/p/doing-stuff-with-ai-opinionated-midyear
https://x.com/dwarkesh_sp/status/1981074799758921843

## ArXiv Papers (569 total)
See notes/arxiv-links.txt for full list
