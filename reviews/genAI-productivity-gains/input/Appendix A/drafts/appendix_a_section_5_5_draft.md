# Appendix A, Section 5.5: Industrial-Organizational (I-O) Psychology

## Overview

I-O psychology applies psychological principles to workplace settings. It provides frameworks for personnel selection, performance appraisal, and training that inform the "evaluate AI like hiring an employee" metaphor sometimes used for AI evaluation.

For AI evaluation, I-O psychology offers insights on defining jobs, selecting workers, evaluating performance, and training—all applicable by analogy to AI systems.

---

## Core Concepts

### Job Analysis

Job analysis systematically studies job requirements:

**Task analysis**: What activities comprise the job? Observable actions and their sequence.

**KSAOs**: What Knowledge, Skills, Abilities, and Other characteristics are needed?

**Critical incidents**: What behaviors distinguish successful from unsuccessful performers?

**Competency modeling**: What competencies are required for effective performance?

For AI, job analysis asks:
- What tasks should AI perform?
- What capabilities are required?
- What distinguishes good from poor AI performance?

This parallels defining what AI should do before evaluating whether it can. Without clear job requirements, evaluation lacks foundation.

### Personnel Selection

Selection research identifies predictors of job performance:

**General cognitive ability** is a strong predictor across jobs (r ≈ 0.5 with performance). Smart people tend to perform better.

**Specific abilities** matter for specific jobs. Technical skills predict technical performance.

**Work samples** predict performance better than general tests. Actual work tasks are the best predictors.

**Structured interviews** outperform unstructured. Structure reduces bias and improves validity.

For AI, selection thinking suggests:
- Evaluate AI on tasks representative of actual use (work samples)
- Don't rely solely on general benchmarks—task-specific evaluation matters
- Structure evaluation to improve reliability

### Validity Generalization

Does selection test validity generalize across contexts?

Meta-analytic evidence shows:
- Some generalization: Cognitive ability predicts performance broadly
- Some context-dependence: Specific skills matter for specific jobs
- Situational factors moderate relationships

For AI, validity generalization asks:
- Does benchmark performance generalize across contexts?
- Evidence suggests limited generalization—the benchmark-to-real gap
- Task and context specificity matters

### The Criterion Problem

What is "good performance"? The criterion problem recognizes this isn't obvious:

**Ultimate criterion**: Theoretical ideal of what performance should capture. Perfect measurement of contribution.

**Actual criterion**: What we can actually measure. Always imperfect proxy for ultimate criterion.

**Criterion deficiency**: What's missing from our measure? What aspects of performance aren't captured?

**Criterion contamination**: What's measured that shouldn't be? What irrelevant factors affect scores?

For AI, the criterion problem is central:
- "Good AI output" isn't well-defined
- Benchmarks may be criterion-deficient (missing important aspects)
- Benchmarks may be contaminated (measuring irrelevant factors)
- The gap between what we measure and what matters may be large

### Structured vs. Unstructured Assessment

Research consistently shows structured assessment outperforms unstructured:

**Structured interviews** beat unstructured interviews. Same questions, scored against criteria.

**Rating scales with behavioral anchors** beat global ratings. Clear criteria, specific levels.

**Multiple raters** beat single raters. Reduces individual bias.

For AI evaluation, structure improves assessment quality:
- Structured rubrics beat unstructured "vibes"
- Defined criteria beat holistic judgments
- Multiple evaluators beat single evaluator
- Consistent procedures beat ad hoc evaluation

### Performance Appraisal

Performance appraisal methods include:

**Rating scales**: Behaviorally anchored rating scales (BARS), behavioral observation scales (BOS).

**Multi-source feedback (360-degree)**: Ratings from supervisors, peers, subordinates, customers.

**Ranking and forced distribution**: Comparing employees to each other.

Appraisal biases include:
- **Halo effect**: One positive aspect inflates all ratings
- **Leniency/severity**: Reluctance to rate at extremes
- **Recency**: Recent events weighted heavily
- **Central tendency**: Clustering around the middle

For AI output evaluation, these biases apply:
- Impressive AI on one dimension may inflate others (halo)
- Evaluators may avoid extreme ratings (central tendency)
- Recent AI outputs may be overweighted (recency)

Structured evaluation with clear criteria reduces these biases.

### Training Evaluation

Kirkpatrick's four levels:

**Level 1: Reaction**: Did participants like the training? Satisfaction measures.

**Level 2: Learning**: Did participants learn the content? Knowledge and skill tests.

**Level 3: Behavior**: Did participants change behavior on the job? Transfer to work.

**Level 4: Results**: Did the organization see results? Business outcomes.

Each level is necessary but not sufficient. Liking training doesn't mean learning; learning doesn't mean behavior change; behavior change doesn't guarantee results.

For AI user training, Kirkpatrick's model applies:
- Do users like AI training? (Reaction)
- Do they learn effective AI use? (Learning)
- Do they use AI differently on the job? (Behavior)
- Do outcomes improve? (Results)

Training evaluation should address all four levels.

---

## What Transfers to AI Evaluation

**Job analysis**: Define what AI should do before evaluating whether it can.

**Work sample testing**: Evaluate on realistic tasks, not just general benchmarks.

**The criterion problem**: Recognize difficulty defining "good" AI output. Measures are always imperfect.

**Structured evaluation**: Use structure to reduce noise and bias. Rubrics beat vibes.

**Training evaluation**: Apply Kirkpatrick to AI training programs.

---

## What Breaks for Generative AI

**Human-centric frameworks**: I-O psychology is about humans; AI is different. Not all concepts map cleanly.

**Stable traits**: I-O psychology assumes relatively stable individual differences. AI capabilities change rapidly.

**Clear criterion**: Employee performance, while complex, is better defined than AI output quality.

**Social dynamics**: Workplace performance involves social factors not applicable to AI.

---

## What Can Be Adapted

**Job analysis for AI**: Systematic study of what tasks AI should perform and what capabilities are needed.

**Work sample evaluation**: Testing AI on realistic tasks representative of intended use.

**Structured rubrics**: Adapted for AI output evaluation with defined criteria and levels.

**Multi-evaluator designs**: Multiple raters to reduce individual bias in AI evaluation.

---

## Implications for AI Evaluation

**Apply job analysis thinking**: What should AI do? What capabilities are needed? Define before evaluating.

**Use work sample evaluation**: Test on realistic tasks, not just standardized benchmarks.

**Apply structured evaluation**: Reduce noise and bias through structure—rubrics, criteria, procedures.

**Recognize the criterion problem**: Defining good output is hard. Acknowledge measures' limitations.

**Train evaluators**: Evaluator training improves assessment quality, just as in performance appraisal.

---

## Key References

- **Schmidt, F.L., & Hunter, J.E. (1998). "The Validity and Utility of Selection Methods in Personnel Psychology." *Psychological Bulletin*.** Meta-analysis of selection methods.

- **Campion, M.A., Palmer, D.K., & Campion, J.E. (1997). "A Review of Structure in the Selection Interview." *Personnel Psychology*.** Structure in assessment.

- **Kirkpatrick, J.D., & Kirkpatrick, W.K. (2016). *Kirkpatrick's Four Levels of Training Evaluation*.** Training evaluation framework.

- **Landy, F.J., & Conte, J.M. (2016). *Work in the 21st Century* (5th ed.).** Comprehensive I-O psychology textbook.

- **Austin, J.T., & Villanova, P. (1992). "The Criterion Problem: 1917-1992." *Journal of Applied Psychology*.** History of the criterion problem.

---

## Connections to Other Sections

I-O psychology connects to several other disciplines covered in this appendix:

- **Section 2.2 (Psychometrics)** provides measurement theory underlying selection and appraisal.

- **Section 5.4 (JDM)** addresses biases in judgment relevant to performance appraisal.

- **Section 5.3 (Organizational Behavior)** addresses organizational context of work performance.

- **Section 6.2 (Program Evaluation)** shares concern for intervention effectiveness.

- **Section 5.1 (HCI)** addresses user skill and training relevant to AI use.

---

*[End of Section 5.5]*
